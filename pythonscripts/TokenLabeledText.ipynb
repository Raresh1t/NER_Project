{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62ddcf42-5245-42c2-b7a7-4218b8f6cccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: transformers in c:\\users\\magnu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.46.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\magnu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\magnu\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\magnu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\magnu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\magnu\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\magnu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\magnu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\magnu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\magnu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\magnu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\magnu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.66.6)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\magnu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\magnu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\magnu\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\magnu\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\magnu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\magnu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\magnu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\magnu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading pandas-2.2.3-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.0/11.6 MB 6.3 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.9/11.6 MB 8.8 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 6.0/11.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.4/11.6 MB 11.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.0/11.6 MB 10.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.7/11.6 MB 9.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.3/11.6 MB 8.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 7.8 MB/s eta 0:00:00\n",
      "Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2024.2 tzdata-2024.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\magnu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas transformers\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "with open('../labeled_data/gold_labels/reconstructed_gold_labels.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "texts = [item['text'] for item in data]\n",
    "entities = [item['entities'] for item in data]\n",
    "df = pd.DataFrame({'text': texts, 'entities': entities})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a163175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nl_positions(text):\n",
    "    nl_pos = []\n",
    "    current = 0\n",
    "    while text.find(\"\\n\", current) != -1:\n",
    "        nl_pos.append(text.find(\"\\n\", current))\n",
    "        current = text.find(\"\\n\", current) +1\n",
    "    return nl_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b42efe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nl_positions'] = df['text'].apply(\n",
    "    lambda x: find_nl_positions(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9bee9c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>entities</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>labels</th>\n",
       "      <th>nl_positions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A new ransomware-as-a-service (RaaS) operation...</td>\n",
       "      <td>[{'start': 716, 'end': 732, 'type': 'ORG', 'te...</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask, of...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[162, 359, 532, 702, 869, 1140, 1238, 1434, 14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The popular Docker-OSX project has been remove...</td>\n",
       "      <td>[{'start': 14, 'end': 24, 'type': 'Software', ...</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask, of...</td>\n",
       "      <td>[O, O, O, O, B-Software, I-Software, I-Softwar...</td>\n",
       "      <td>[182, 410, 580, 701, 847, 1101, 1287, 1444, 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A former core infrastructure engineer at an in...</td>\n",
       "      <td>[{'start': 82, 'end': 97, 'type': 'LOC', 'text...</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask, of...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC,...</td>\n",
       "      <td>[220, 523, 751, 1135, 1349, 1721, 2063, 2263, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Threat actors target Middle Eastern organizati...</td>\n",
       "      <td>[{'start': 23, 'end': 37, 'type': 'LOC', 'text...</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask, of...</td>\n",
       "      <td>[O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, O,...</td>\n",
       "      <td>[219, 519, 681, 870, 1012, 1163, 1368, 1493, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Since surfacing in February 2024, RansomHub ra...</td>\n",
       "      <td>[{'start': 37, 'end': 46, 'type': 'MAL-ORG', '...</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask, of...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-MAL-ORG, I...</td>\n",
       "      <td>[155, 528, 914, 1040, 1298, 1546, 1982, 1983, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>​Russian law enforcement detained almost 100 s...</td>\n",
       "      <td>[{'start': 2, 'end': 10, 'type': 'LOC', 'text'...</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask, of...</td>\n",
       "      <td>[O, O, O, B-Event, I-Event, I-Event, I-Event, ...</td>\n",
       "      <td>[238, 463, 786, 1075, 1240, 1471, 1718, 2024, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>The Russian state-sponsored APT29 hacking grou...</td>\n",
       "      <td>[{'start': 5, 'end': 13, 'type': 'LOC', 'text'...</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask, of...</td>\n",
       "      <td>[O, O, O, O, O, O, O, B-MAL-ORG, I-MAL-ORG, O,...</td>\n",
       "      <td>[208, 388, 523, 734, 971, 1081, 1312, 1485, 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>The South Korea-aligned cyberespionage group A...</td>\n",
       "      <td>[{'start': 6, 'end': 25, 'type': 'LOC', 'text'...</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask, of...</td>\n",
       "      <td>[O, O, O, B-LOC, I-LOC, I-LOC, I-LOC, O, O, O,...</td>\n",
       "      <td>[206, 365, 570, 798, 973, 1177, 1331, 1516, 17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>The Federal Communications Commission (FCC) an...</td>\n",
       "      <td>[{'start': 6, 'end': 39, 'type': 'ORG', 'text'...</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask, of...</td>\n",
       "      <td>[O, O, O, B-ORG, I-ORG, I-ORG, O, B-ORG, O, B-...</td>\n",
       "      <td>[195, 435, 609, 830, 1055, 1224, 1449, 1868, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Nuclear waste processing facility Sellafield h...</td>\n",
       "      <td>[{'start': 34, 'end': 44, 'type': 'ORG', 'text...</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask, of...</td>\n",
       "      <td>[B-Event, I-Event, I-Event, I-Event, I-Event, ...</td>\n",
       "      <td>[254, 483, 697, 936, 1183, 1322, 1574, 1709, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "0   A new ransomware-as-a-service (RaaS) operation...   \n",
       "1   The popular Docker-OSX project has been remove...   \n",
       "2   A former core infrastructure engineer at an in...   \n",
       "3   Threat actors target Middle Eastern organizati...   \n",
       "4   Since surfacing in February 2024, RansomHub ra...   \n",
       "..                                                ...   \n",
       "56  ​Russian law enforcement detained almost 100 s...   \n",
       "57  The Russian state-sponsored APT29 hacking grou...   \n",
       "58  The South Korea-aligned cyberespionage group A...   \n",
       "59  The Federal Communications Commission (FCC) an...   \n",
       "60  Nuclear waste processing facility Sellafield h...   \n",
       "\n",
       "                                             entities  \\\n",
       "0   [{'start': 716, 'end': 732, 'type': 'ORG', 'te...   \n",
       "1   [{'start': 14, 'end': 24, 'type': 'Software', ...   \n",
       "2   [{'start': 82, 'end': 97, 'type': 'LOC', 'text...   \n",
       "3   [{'start': 23, 'end': 37, 'type': 'LOC', 'text...   \n",
       "4   [{'start': 37, 'end': 46, 'type': 'MAL-ORG', '...   \n",
       "..                                                ...   \n",
       "56  [{'start': 2, 'end': 10, 'type': 'LOC', 'text'...   \n",
       "57  [{'start': 5, 'end': 13, 'type': 'LOC', 'text'...   \n",
       "58  [{'start': 6, 'end': 25, 'type': 'LOC', 'text'...   \n",
       "59  [{'start': 6, 'end': 39, 'type': 'ORG', 'text'...   \n",
       "60  [{'start': 34, 'end': 44, 'type': 'ORG', 'text...   \n",
       "\n",
       "                                            tokenized  \\\n",
       "0   [input_ids, token_type_ids, attention_mask, of...   \n",
       "1   [input_ids, token_type_ids, attention_mask, of...   \n",
       "2   [input_ids, token_type_ids, attention_mask, of...   \n",
       "3   [input_ids, token_type_ids, attention_mask, of...   \n",
       "4   [input_ids, token_type_ids, attention_mask, of...   \n",
       "..                                                ...   \n",
       "56  [input_ids, token_type_ids, attention_mask, of...   \n",
       "57  [input_ids, token_type_ids, attention_mask, of...   \n",
       "58  [input_ids, token_type_ids, attention_mask, of...   \n",
       "59  [input_ids, token_type_ids, attention_mask, of...   \n",
       "60  [input_ids, token_type_ids, attention_mask, of...   \n",
       "\n",
       "                                               labels  \\\n",
       "0   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1   [O, O, O, O, B-Software, I-Software, I-Softwar...   \n",
       "2   [O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC,...   \n",
       "3   [O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, O,...   \n",
       "4   [O, O, O, O, O, O, O, O, O, O, O, B-MAL-ORG, I...   \n",
       "..                                                ...   \n",
       "56  [O, O, O, B-Event, I-Event, I-Event, I-Event, ...   \n",
       "57  [O, O, O, O, O, O, O, B-MAL-ORG, I-MAL-ORG, O,...   \n",
       "58  [O, O, O, B-LOC, I-LOC, I-LOC, I-LOC, O, O, O,...   \n",
       "59  [O, O, O, B-ORG, I-ORG, I-ORG, O, B-ORG, O, B-...   \n",
       "60  [B-Event, I-Event, I-Event, I-Event, I-Event, ...   \n",
       "\n",
       "                                         nl_positions  \n",
       "0   [162, 359, 532, 702, 869, 1140, 1238, 1434, 14...  \n",
       "1   [182, 410, 580, 701, 847, 1101, 1287, 1444, 16...  \n",
       "2   [220, 523, 751, 1135, 1349, 1721, 2063, 2263, ...  \n",
       "3   [219, 519, 681, 870, 1012, 1163, 1368, 1493, 1...  \n",
       "4   [155, 528, 914, 1040, 1298, 1546, 1982, 1983, ...  \n",
       "..                                                ...  \n",
       "56  [238, 463, 786, 1075, 1240, 1471, 1718, 2024, ...  \n",
       "57  [208, 388, 523, 734, 971, 1081, 1312, 1485, 16...  \n",
       "58  [206, 365, 570, 798, 973, 1177, 1331, 1516, 17...  \n",
       "59  [195, 435, 609, 830, 1055, 1224, 1449, 1868, 2...  \n",
       "60  [254, 483, 697, 936, 1183, 1322, 1574, 1709, 1...  \n",
       "\n",
       "[61 rows x 5 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5645404d-0e2d-41be-8f21-c3c4f210a769",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "df['tokenized'] = df['text'].apply(\n",
    "    lambda x: tokenizer(x, return_offsets_mapping=True, truncation=True, padding=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "53b7e5fb-eae0-4526-9d69-037431c75f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_to_tokens(text, entities, tokenized, nl_positions):\n",
    "    offset_mapping = tokenized['offset_mapping']\n",
    "    labels = [\"O\"] * len(offset_mapping) # Initialize all tokens with \"O\"\n",
    "\n",
    "    for entity in entities:\n",
    "        start, end, label_type = entity['start'], entity['end'], entity['type']\n",
    "        nls_before_entity = 0\n",
    "        for nl_pos in nl_positions:\n",
    "            if nl_pos < start:\n",
    "                nls_before_entity +=1\n",
    "            else:\n",
    "                break\n",
    "        start -= nls_before_entity\n",
    "        end -= nls_before_entity\n",
    "        entity_started = False\n",
    "        #print(entity)\n",
    "\n",
    "        for idx, (token_start, token_end) in enumerate(offset_mapping):\n",
    "            #print(token_start, token_end)\n",
    "            if token_start is None or token_end is None:\n",
    "                continue\n",
    "            if token_start >= start and token_end <= end:\n",
    "                if entity_started:\n",
    "                    labels[idx] = f\"I-{label_type}\"\n",
    "                else:\n",
    "                    labels[idx] = f\"B-{label_type}\"\n",
    "                    entity_started = True\n",
    "            else:\n",
    "                entity_started = False\n",
    "\n",
    "    return labels\n",
    "\n",
    "df['labels'] = df.apply(lambda row: align_labels_to_tokens(row['text'], row['entities'], row['tokenized'], row['nl_positions']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ad751a07-b0af-4036-9da7-d9eac01ed409",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Save to JSON or CSV format\n",
    "output_data = []\n",
    "for _, row in df.iterrows():\n",
    "    tokens = tokenizer.convert_ids_to_tokens(row['tokenized']['input_ids'])\n",
    "    labels = row['labels']\n",
    "    output_data.append({'tokens': tokens, 'labels': labels})\n",
    "\n",
    "# Save the processed data\n",
    "with open('tokenized_ner_data_3.json', 'w') as f:\n",
    "    json.dump(output_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f946a71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\n",
      "##ica\n",
      "##da\n",
      "##33\n",
      "##01\n",
      "##da\n",
      "##33\n",
      "##01\n",
      "Ra\n",
      "##ping\n",
      "##C\n",
      "##om\n",
      "##pute\n",
      "##r\n",
      "is\n",
      "##da\n",
      "##33\n",
      "##01\n",
      "double\n",
      "-\n",
      "ex\n",
      "##tor\n",
      "##tion\n",
      "tactics\n",
      "where\n",
      "they\n",
      "breach\n",
      "corporate\n",
      "networks\n",
      ",\n",
      "steal\n",
      "data\n",
      ",\n",
      "and\n",
      "then\n",
      "en\n",
      "##c\n",
      "##ry\n",
      "##pt\n",
      "devices\n",
      ".\n",
      "The\n",
      "##01\n",
      "and\n",
      "AL\n",
      "##C\n",
      ",\n",
      "core\n",
      "from\n",
      "one\n",
      "also\n",
      "ransom\n",
      "##net\n",
      "for\n",
      "##inet\n",
      "Pa\n",
      "##lo\n",
      "and\n",
      "down\n",
      "a\n",
      "R\n",
      "##ust\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "for i, label in enumerate(output_data[0]['labels']):\n",
    "    if label != \"O\":\n",
    "        print(output_data[0]['tokens'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8be507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nl_positions(text):\n",
    "    nl_pos = []\n",
    "    current = 0\n",
    "    while text.find(\"\\n\", current) != -1:\n",
    "        nl_pos.append(text.find(\"\\n\", current))\n",
    "        current = text.find(\"\\n\", current) +1\n",
    "    return nl_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d731d754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[162,\n",
       " 359,\n",
       " 532,\n",
       " 702,\n",
       " 869,\n",
       " 1140,\n",
       " 1238,\n",
       " 1434,\n",
       " 1467,\n",
       " 1683,\n",
       " 1996,\n",
       " 2172,\n",
       " 2388,\n",
       " 2671,\n",
       " 2850,\n",
       " 3076,\n",
       " 3294,\n",
       " 3624,\n",
       " 3755,\n",
       " 3886,\n",
       " 4049,\n",
       " 4273,\n",
       " 4457,\n",
       " 4690,\n",
       " 4747,\n",
       " 4806,\n",
       " 4868,\n",
       " 4939]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nl_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "37e5b07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = align_labels_to_tokens(text, df.iloc[0]['entities'], df.iloc[0]['tokenized'], nl_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ecfdb6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\n",
      "##ica\n",
      "##da\n",
      "##33\n",
      "##01\n",
      "C\n",
      "##ica\n",
      "##da\n",
      "##33\n",
      "##01\n",
      "B\n",
      "##lee\n",
      "##ping\n",
      "##C\n",
      "##om\n",
      "##pute\n",
      "##r\n",
      "C\n",
      "##ica\n",
      "##da\n",
      "C\n",
      "##ica\n",
      "##da\n",
      "##33\n",
      "##01\n",
      "conducts\n",
      "double\n",
      "-\n",
      "ex\n",
      "##tor\n",
      "##tion\n",
      "tactics\n",
      "where\n",
      "they\n",
      "breach\n",
      "corporate\n",
      "networks\n",
      ",\n",
      "steal\n",
      "data\n",
      ",\n",
      "and\n",
      "then\n",
      "en\n",
      "##c\n",
      "##ry\n",
      "##pt\n",
      "devices\n",
      "True\n",
      "##se\n",
      "##c\n",
      "C\n",
      "##ica\n",
      "##da\n",
      "##33\n",
      "##01\n",
      "AL\n",
      "##P\n",
      "##H\n",
      "##V\n",
      "Black\n",
      "##C\n",
      "##at\n",
      "AL\n",
      "##P\n",
      "##H\n",
      "##V\n",
      "'\n",
      "s\n",
      "AL\n",
      "##P\n",
      "##H\n",
      "##V\n",
      "FBI\n",
      "Change\n",
      "Healthcare\n",
      "True\n",
      "##se\n",
      "##c\n",
      "C\n",
      "##ica\n",
      "##da\n",
      "##33\n",
      "##01\n",
      "B\n",
      "##ru\n",
      "##tus\n",
      "b\n",
      "##ot\n",
      "##net\n",
      "C\n",
      "##isco\n",
      "Fort\n",
      "##inet\n",
      "Pa\n",
      "##lo\n",
      "Alto\n",
      "Sonic\n",
      "##W\n",
      "##all\n",
      "B\n",
      "##ru\n",
      "##tus\n",
      "AL\n",
      "##P\n",
      "##H\n",
      "##V\n",
      "C\n",
      "##ica\n",
      "##da\n",
      "##33\n",
      "##01\n"
     ]
    }
   ],
   "source": [
    "for i, label in enumerate(labels):\n",
    "    if label != \"O\":\n",
    "        print(output_data[0]['tokens'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef3469b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize tokenizer (replace with your specific tokenizer)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_from_tokens(text, entities):\n",
    "    \"\"\"\n",
    "    Identify and group continuous tokens into a single entity based on start and end indexes.\n",
    "\n",
    "    :param text: Input text for tokenization.\n",
    "    :param entities: List of entities with start, end, type, and text fields.\n",
    "    :return: List of merged entities with aligned start and end positions.\n",
    "    \"\"\"\n",
    "    # Tokenize the text with offset mappings\n",
    "    tokenized_text = tokenizer(text, return_offsets_mapping=True)\n",
    "    tokens = tokenized_text.input_ids\n",
    "    offsets = tokenized_text.offset_mapping\n",
    "    \n",
    "    # Initialize variables to store the results\n",
    "    gold_entities = []\n",
    "    index = 0\n",
    "    entity = entities[index] if entities else None\n",
    "    nextent = entities[index+1]\n",
    "    temp_text = \"\"  # Temporary storage for concatenating tokens within the same entity span\n",
    "\n",
    "    # Loop through each token and check if it aligns with an entity\n",
    "    for i in range(len(tokens)):\n",
    "        print(f\"i={i}\")\n",
    "        if not entity:\n",
    "            break  # Exit if no more entities\n",
    "\n",
    "        # Check if the token's start aligns with the current entity's start\n",
    "        # Offsets[i][0] is the offset_mapping start index.\n",
    "        # If it is equal to the entities[index]['start'] it will enter the loop\n",
    "        if offsets[i][0] == entity['start']:\n",
    "            print(f\"Offsets: {offsets[i][0]} + Entity start index:{entity['start']}\")\n",
    "            # Accumulate token text within the entity\n",
    "            temp_text += tokenizer.decode([tokens[i]], skip_special_tokens=True).replace(\"##\", \"\")\n",
    "            print(f\"Temp text: {temp_text}\")\n",
    "            start_token = entity['start']\n",
    "            #print(f\"Start token: {start_token}\")\n",
    "            # Create a single merged entity for the accumulated text\n",
    "            datapoint = {\n",
    "                \"text\": temp_text,           # Combined text of all sub-tokens\n",
    "                \"type\": entity['type'],      # Entity type\n",
    "                \"start\": entity['start'],    # Entity start position\n",
    "                \"end\": entity['end']         # Entity end position\n",
    "            }\n",
    "            gold_entities.append(datapoint)\n",
    "            if offsets[i][1] == nextent['start']:\n",
    "                print(f\"This text says that offsets[i][1]: {offsets[i][1]} is equal to next entity start: {nextent['start']}\")\n",
    "            # Reset for the next entity\n",
    "            index += 1\n",
    "            entity = entities[index] if index < len(entities) else None\n",
    "    print(f\"Gold entities:{gold_entities}\\n\")\n",
    "    return gold_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_extract_entities_from_tokens(text, entities):\n",
    "    \"\"\"\n",
    "    Identify and group continuous tokens into a single entity based on start and end indexes.\n",
    "\n",
    "    :param text: Input text for tokenization.\n",
    "    :param entities: List of entities with start, end, type, and text fields.\n",
    "    :return: List of merged entities with aligned start and end positions.\n",
    "    \"\"\"\n",
    "    # Tokenize the text with offset mappings\n",
    "    tokenized_text = tokenizer(text, return_offsets_mapping=True)\n",
    "    tokens = tokenized_text.input_ids\n",
    "    offsets = tokenized_text.offset_mapping\n",
    "    \n",
    "    # Initialize variables to store the results\n",
    "    merged_entities = []\n",
    "    temp_text = \"\" # Temporary storage for concatenating tokens within the same entity span\n",
    "    temp_types = [] # Temporary storage for entity types\n",
    "    start_pos = None # Start pos of curr entity\n",
    "    end_pos = None # End pos for current entity\n",
    "\n",
    "    # Loop through each token and check if it aligns with an entity\n",
    "    for entity in entities:\n",
    "        token_text = entity[\"text\"].replace(\"##\", \"\")\n",
    "        print(f\"token text: {token_text}\")\n",
    "        if not temp_text or entity[\"start\"] != end_pos:\n",
    "            if temp_text:\n",
    "                majority_type = max(set(temp_types), key=temp_types.count)\n",
    "                merged_entities.append({\n",
    "                    \"text\": temp_text,\n",
    "                    \"type\": majority_type,\n",
    "                    \"start\": start_pos,\n",
    "                    \"end\": end_pos\n",
    "                })\n",
    "            temp_text = token_text\n",
    "            temp_types = [entity[\"type\"]]\n",
    "            start_pos = entity[\"start\"]\n",
    "            end_pos = entity[\"end\"]\n",
    "        else:\n",
    "            temp_text += token_text\n",
    "            temp_types.append(entity[\"type\"])\n",
    "            end_pos = entity[\"end\"]\n",
    "    if temp_text:\n",
    "        majority_type = max(set(temp_types), key=temp_types.count)\n",
    "        merged_entities.append({\n",
    "            \"text\": temp_text,\n",
    "            \"type\": majority_type,\n",
    "            \"start\": start_pos,\n",
    "            \"end\": end_pos\n",
    "        })\n",
    "    return merged_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A new ransomware-as-a-service (RaaS) operation named Cicada3301 has already listed 19 victims on its extortion portal.\"\n",
    "entities = [\n",
    "            {\n",
    "                \"start\": 53,\n",
    "                \"end\": 54,\n",
    "                \"type\": \"I-ORG\",\n",
    "                \"text\": \"C\"\n",
    "            },\n",
    "            {\n",
    "                \"start\": 54,\n",
    "                \"end\": 57,\n",
    "                \"type\": \"I-MISC\",\n",
    "                \"text\": \"##ica\"\n",
    "            },\n",
    "            {\n",
    "                \"start\": 57,\n",
    "                \"end\": 59,\n",
    "                \"type\": \"I-MISC\",\n",
    "                \"text\": \"##da\"\n",
    "            },\n",
    "            {\n",
    "                \"start\": 59,\n",
    "                \"end\": 61,\n",
    "                \"type\": \"I-ORG\",\n",
    "                \"text\": \"##33\"\n",
    "            },\n",
    "            {\n",
    "                \"start\": 61,\n",
    "                \"end\": 63,\n",
    "                \"type\": \"I-ORG\",\n",
    "                \"text\": \"##01\"\n",
    "            },\n",
    "            {\n",
    "                \"start\":65,\n",
    "                \"end\": 67,\n",
    "                \"type\": \"O\",\n",
    "                \"text\": \"test\"\n",
    "            },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 138, 1207, 25057, 7109, 118, 1112, 118, 170, 118, 1555, 113, 16890, 1161, 1708, 114, 2805, 1417, 140, 4578, 1810, 23493, 24400, 1144, 1640, 2345, 1627, 5256, 1113, 1157, 4252, 2772, 2116, 10823, 119, 102]\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 53, 'end': 54, 'type': 'I-ORG', 'text': 'C'}\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "print(entities[0])\n",
    "print(entities[0]['start'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False because: 65 not equal 63\n"
     ]
    }
   ],
   "source": [
    "entity1 = entities[4+1]['start']\n",
    "entity2 = entities[4]['end']\n",
    "if entity1 == entity2:\n",
    "    print(f\"True because: {entity1} = {entity2}\")\n",
    "else:\n",
    "    print(f\"False because: {entity1} not equal {entity2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 53, 'end': 54, 'type': 'I-ORG', 'text': 'C'}\n",
      "{'start': 54, 'end': 57, 'type': 'I-MISC', 'text': '##ica'}\n",
      "{'start': 57, 'end': 59, 'type': 'I-MISC', 'text': '##da'}\n",
      "{'start': 59, 'end': 61, 'type': 'I-ORG', 'text': '##33'}\n",
      "{'start': 61, 'end': 63, 'type': 'I-ORG', 'text': '##01'}\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(entities[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token text: C\n",
      "token text: ica\n",
      "token text: da\n",
      "token text: 33\n",
      "token text: 01\n",
      "token text: test\n",
      "[{'text': 'Cicada3301', 'type': 'I-ORG', 'start': 53, 'end': 63}, {'text': 'test', 'type': 'O', 'start': 65, 'end': 67}]\n"
     ]
    }
   ],
   "source": [
    "token_aligned_entities = new_extract_entities_from_tokens(text, entities)\n",
    "print(token_aligned_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

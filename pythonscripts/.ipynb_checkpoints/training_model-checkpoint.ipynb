{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flash-attn\n",
      "  Using cached flash_attn-2.7.4.post1.tar.gz (6.0 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\magnu\\\\AppData\\\\Local\\\\Temp\\\\pip-install-_h0lj359\\\\flash-attn_a64e364bab4b4bda827435c2541f476a\\\\csrc\\\\composable_kernel\\\\library\\\\include\\\\ck\\\\library\\\\tensor_operation_instance\\\\gpu\\\\grouped_conv_bwd_weight\\\\device_grouped_conv_bwd_weight_two_stage_xdl_instance.hpp'\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#! pip install seqeval\n",
    "#! pip install evaluate\n",
    "#! pip install pandas\n",
    "#! pip install datasets\n",
    "#! pip install torch\n",
    "#! pip install transformers\n",
    "#! pip install scikit-learn\n",
    "#! pip install ninja\n",
    "#! pip install flash-attn\n",
    "#! pip install packaging\n",
    "#! pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "#import evaluate\n",
    "#import seqeval\n",
    "#import accelerate\n",
    "import transformers\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from flash_attn.flash_attention import FlashAttention\n",
    "# Flash Attention for faster training\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # Enable TF32 for better performance\n",
    "torch.backends.cudnn.benchmark = True  # Enable CuDNN auto-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "DATA_PATH = \"tokenized_ner_data_4.json\"\n",
    "with open(DATA_PATH, \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all unique label types from dataset\n",
    "unique_labels = list(set(label for entry in data for label in entry[\"labels\"]))\n",
    "\n",
    "# Create mapping from label name -> index\n",
    "label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
    "\n",
    "# Convert dataset labels from strings to integer IDs\n",
    "for entry in data:\n",
    "    entry[\"labels\"] = [label_to_id[label] for label in entry[\"labels\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    tokenized_inputs = []\n",
    "    tokenized_labels = []\n",
    "\n",
    "    for entry in data:\n",
    "        tokens = entry[\"tokens\"]\n",
    "        labels = entry[\"labels\"]\n",
    "\n",
    "        tokenized_inputs.append(tokens)\n",
    "        tokenized_labels.append(labels)\n",
    "    return {\"tokens\": tokenized_inputs, \"labels\": tokenized_labels}\n",
    "\n",
    "dataset = Dataset.from_dict(preprocess_data(data))\n",
    "train_dataset, eval_dataset = dataset.train_test_split(test_size=0.2).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a98a3b5fdf489fb6eed538a1c940c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/20.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magnu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\magnu\\.cache\\huggingface\\hub\\models--answerdotai--ModernBERT-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f34a5df9ad4fdf8aae35205e2bd542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.13M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "504f6d9ff0e145d39985730659b9c4f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "MODEL_NAME = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "#model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, num_labels=len(set([label for entry in data for label in entry[\"labels\"]]))).to(device)\n",
    "\n",
    "# Enable Flash Attention in the model (if applicable)\n",
    "#model.config.use_flash_attention = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b60ed266604da9a7d6ee7f22aa9631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/61 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Sample Debugging ====\n",
      "Len examples:  512\n",
      "Len tokenized inputs:  692\n",
      "Original Tokens:  ['[CLS]', 'Nuclear', 'waste', 'processing', 'facility', 'Se', '##lla', '##field', 'has', 'been', 'fined', 'Â£', '##33', '##2', ',', '500', '(', '$', '440', '##k', ')', 'by', 'the', 'Office', 'for', 'Nuclear', 'Regulation', '(', 'ON', '##R', ')', 'for', 'failing', 'to', 'ad', '##here', 'to', 'c', '##y', '##bers', '##ec', '##urity', 'standards', 'and', 'putting', 'sensitive', 'nuclear', 'information', 'at', 'risk', 'over', 'four', 'years', ',', 'from', '2019', 'to', '202', '##3', '.', 'According', 'to', 'the', 'ON', '##R', 'announcement', ',', 'Se', '##lla', '##field', 'failed', 'to', 'follow', 'its', 'own', 'approved', 'c', '##y', '##bers', '##ec', '##urity', 'protocols', 'by', 'leaving', 'multiple', 'v', '##ul', '##ner', '##abi', '##lities', 'in', 'its', 'IT', 'systems', 'un', '##pa', '##tched', ',', 'violating', 'the', 'Nuclear', 'Industries', 'Security', 'Regulations', '2003', '.', 'Although', 'no', 'exploitation', 'has', 'occurred', ',', 'the', 'weaknesses', 'exposed', 'the', 'facility', 'to', 'risks', 'such', 'as', 'ransom', '##ware', ',', 'p', '##his', '##hing', ',', 'and', 'potential', 'data', 'loss', ',', 'which', 'could', 'disrupt', 'high', '-', 'hazard', 'operations', 'and', 'delay', 'de', '##com', '##mission', '##ing', 'work', '.', 'Se', '##lla', '##field', 'is', 'one', 'of', 'Europe', \"'\", 's', 'largest', 'nuclear', 'facilities', ',', 'located', 'in', 'C', '##umbria', ',', 'UK', '.', 'It', 'plays', 'a', 'significant', 'role', 'in', 'managing', 'and', 'processing', 'radioactive', 'materials', ',', 'handling', 'more', 'nuclear', 'waste', 'in', 'one', 'location', 'than', 'any', 'other', 'facility', 'worldwide', '.', 'The', 'site', 'is', 'involved', 'in', 're', '##tri', '##ev', '##ing', 'nuclear', 'waste', ',', 'fuel', ',', 'and', 's', '##lu', '##dge', 'from', 'legacy', 'ponds', 'and', 'si', '##los', ',', 'storing', 'radioactive', 'materials', 'such', 'as', 'p', '##lut', '##onium', 'and', 'uranium', ',', 'managing', 'spent', 'nuclear', 'fuel', 'rods', ',', 'and', 're', '##media', '##ting', 'and', 'de', '##com', '##mission', '##ing', 'nuclear', 'facilities', '.', 'Se', '##lla', '##field', 'is', 'a', 'critical', 'unit', 'for', 'the', 'UK', \"'\", 's', 'nuclear', 'waste', 'management', 'system', ',', 'so', 'its', 'IT', 'systems', 'security', 'is', 'vital', 'to', 'ensure', 'safe', 'operations', '.', 'Last', 'year', ',', 'a', 'series', 'of', 'investigations', 'by', 'The', 'Guardian', 'into', 'Se', '##lla', '##field', \"'\", 's', 'c', '##y', '##bers', '##ec', '##urity', 'brought', 'attention', 'to', 'multiple', 'severe', 'issues', ',', 'revealing', 'that', 'contractors', 'had', 'easy', 'access', 'to', 'critical', 'systems', 'where', 'they', ',', 'among', 'other', 'things', ',', 'could', 'install', 'USB', 'drives', '.', 'Additionally', ',', 'well', '-', 'known', 'v', '##ul', '##ner', '##abi', '##lities', 'within', 'the', 'facility', 'a', '##bound', ',', 'giving', 'the', 'site', 'the', 'nickname', '\"', 'Vol', '##de', '##mor', '##t', '\"', 'by', 'people', 'working', 'there', '.', 'An', 'audit', 'from', 'French', 'security', 'firm', 'At', '##os', 'revealed', 'that', 'roughly', '75', '%', 'of', 'Se', '##lla', '##field', \"'\", 's', 'servers', 'were', 'vulnerable', 'to', 'attacks', 'with', 'potentially', 'catastrophic', 'consequences', '.', 'The', 'nuclear', 'site', \"'\", 's', 'operators', 'pleaded', 'guilty', 'in', 'June', '202', '##4', 'to', 'their', 'failure', 'to', 'comply', 'with', 'standard', 'IT', 'security', 'regulations', ',', 'admitting', 'their', 'failure', '.', 'ON', '##R', 'investigated', 'these', 'reports', ',', 'and', 'while', 'it', 'confirmed', 'that', 'Se', '##lla', '##field', 'failed', 'to', 'a', '##bide', 'by', 'the', 'c', '##y', '##bers', '##ec', '##urity', 'standards', 'that', 'under', '##pin', 'the', 'operation', 'of', 'such', 'sites', 'in', 'the', 'UK', ',', 'it', 'says', 'it', 'found', 'no', 'evidence', 'that', 'the', 'v', '##ul', '##ner', '##abi', '##lities', 'were', 'leverage', '##d', 'in', 'attacks', '.', 'This', 'contrasts', 'previous', 'reports', 'by', 'the', 'press', 'that', 'Russian', 'and', 'Chinese', 'ha', '##ckers', 'allegedly', 'planted', 'ma', '##l', '##ware', 'on', 'the', 'site', ',', 'and', 'that', 'security', 'breach', '##es', 'occurred', 'as', 'far', 'back', 'as', '2015', '.', '\"', 'An', 'investigation', 'by', 'ON', '##R', '[', '[SEP]']\n",
      "decoded:  ['[CLS]', '[CLS]', 'N', 'uclear', 'was', 'te', 'processing', 'fac', 'ility', 'Se', '##', 'lla', '##', 'field', 'has', 'been', 'f', 'ined', 'Â£', '##', '33', '##', '2', ',', '500', '(', '$', '440', '##', 'k', ')', 'by', 'the', 'Office', 'for', 'N', 'uclear', 'Reg', 'ulation', '(', 'ON', '##', 'R', ')', 'for', 'f', 'ailing', 'to', 'ad', '##', 'here', 'to', 'c', '##', 'y', '##', 'bers', '##', 'ec', '##', 'urity', 'stand', 'ards', 'and', 'put', 'ting', 'sensitive', 'nuclear', 'information', 'at', 'risk', 'over', 'four', 'years', ',', 'from', '2019', 'to', '202', '##', '3', '.', 'According', 'to', 'the', 'ON', '##', 'R', 'ann', 'ounce', 'ment', ',', 'Se', '##', 'lla', '##', 'field', 'failed', 'to', 'follow', 'its', 'own', 'approved', 'c', '##', 'y', '##', 'bers', '##', 'ec', '##', 'urity', 'prot', 'ocols', 'by', 'le', 'aving', 'multiple', 'v', '##', 'ul', '##', 'ner', '##', 'abi', '##', 'l', 'ities', 'in', 'its', 'IT', 'systems', 'un', '##', 'pa', '##', 't', 'ched', ',', 'viol', 'ating', 'the', 'N', 'uclear', 'Indust', 'ries', 'Security', 'Reg', 'ulations', '2003', '.', 'Although', 'no', 'expl', 'o', 'itation', 'has', 'occur', 'red', ',', 'the', 'weak', 'ness', 'es', 'exposed', 'the', 'fac', 'ility', 'to', 'ris', 'ks', 'such', 'as', 'rans', 'om', '##', 'ware', ',', 'p', '##', 'his', '##', 'hing', ',', 'and', 'potential', 'data', 'loss', ',', 'which', 'could', 'dis', 'rupt', 'high', '-', 'h', 'azard', 'operations', 'and', 'delay', 'de', '##', 'com', '##', 'mission', '##', 'ing', 'work', '.', 'Se', '##', 'lla', '##', 'field', 'is', 'one', 'of', 'Europe', \"'\", 's', 'largest', 'nuclear', 'fac', 'ilities', ',', 'located', 'in', 'C', '##', 'umb', 'ria', ',', 'UK', '.', 'It', 'plays', 'a', 'significant', 'role', 'in', 'man', 'aging', 'and', 'processing', 'radio', 'active', 'materials', ',', 'handling', 'more', 'nuclear', 'was', 'te', 'in', 'one', 'location', 'than', 'any', 'other', 'fac', 'ility', 'world', 'wide', '.', 'The', 'site', 'is', 'inv', 'olved', 'in', 're', '##', 'tri', '##', 'ev', '##', 'ing', 'nuclear', 'was', 'te', ',', 'fuel', ',', 'and', 's', '##', 'lu', '##', 'd', 'ge', 'from', 'leg', 'acy', 'p', 'onds', 'and', 'si', '##', 'los', ',', 'st', 'oring', 'radio', 'active', 'materials', 'such', 'as', 'p', '##', 'lut', '##', 'onium', 'and', 'ur', 'anium', ',', 'man', 'aging', 'sp', 'ent', 'nuclear', 'fuel', 'ro', 'ds', ',', 'and', 're', '##', 'media', '##', 'ting', 'and', 'de', '##', 'com', '##', 'mission', '##', 'ing', 'nuclear', 'fac', 'ilities', '.', 'Se', '##', 'lla', '##', 'field', 'is', 'a', 'critical', 'unit', 'for', 'the', 'UK', \"'\", 's', 'nuclear', 'was', 'te', 'management', 'system', ',', 'so', 'its', 'IT', 'systems', 'security', 'is', 'v', 'ital', 'to', 'ensure', 'safe', 'operations', '.', 'Last', 'year', ',', 'a', 'series', 'of', 'invest', 'ig', 'ations', 'by', 'The', 'Guard', 'ian', 'into', 'Se', '##', 'lla', '##', 'field', \"'\", 's', 'c', '##', 'y', '##', 'bers', '##', 'ec', '##', 'urity', 'br', 'ought', 'attention', 'to', 'multiple', 'severe', 'issues', ',', 'reve', 'aling', 'that', 'contract', 'ors', 'had', 'easy', 'access', 'to', 'critical', 'systems', 'where', 'they', ',', 'among', 'other', 'things', ',', 'could', 'install', 'USB', 'driv', 'es', '.', 'Additionally', ',', 'well', '-', 'known', 'v', '##', 'ul', '##', 'ner', '##', 'abi', '##', 'l', 'ities', 'within', 'the', 'fac', 'ility', 'a', '##', 'bound', ',', 'giving', 'the', 'site', 'the', 'nick', 'name', '\"', 'Vol', '##', 'de', '##', 'mor', '##', 't', '\"', 'by', 'people', 'working', 'there', '.', 'An', 'aud', 'it', 'from', 'French', 'security', 'firm', 'At', '##', 'os', 'reve', 'aled', 'that', 'rough', 'ly', '75', '%', 'of', 'Se', '##', 'lla', '##', 'field', \"'\", 's', 'ser', 'vers', 'were', 'v', 'ul', 'ner', 'able', 'to', 'att', 'acks', 'with', 'pot', 'entially', 'cat', 'ast', 'rophic', 'con', 'sequences', '.', 'The', 'nuclear', 'site', \"'\", 's', 'oper', 'ators', 'ple', 'aded', 'gu', 'ilty', 'in', 'June', '202', '##', '4', 'to', 'their', 'failure', 'to', 'com', 'ply', 'with', 'standard', 'IT', 'security', 'reg', 'ulations', ',', 'ad', 'mitting', 'their', 'failure', '.', 'ON', '##', 'R', 'invest', 'igated', 'these', 'reports', ',', 'and', 'while', 'it', 'confirmed', 'that', 'Se', '##', 'lla', '##', 'field', 'failed', 'to', 'a', '##', 'b', 'ide', 'by', 'the', 'c', '##', 'y', '##', 'bers', '##', 'ec', '##', 'urity', 'stand', 'ards', 'that', 'under', '##', 'pin', 'the', 'operation', 'of', 'such', 'sites', 'in', 'the', 'UK', ',', 'it', 's', 'ays', 'it', 'found', 'no', 'evidence', 'that', 'the', 'v', '##', 'ul', '##', 'ner', '##', 'abi', '##', 'l', 'ities', 'were', 'le', 'verage', '##', 'd', 'in', 'att', 'acks', '.', 'This', 'contr', 'asts', 'previous', 'reports', 'by', 'the', 'press', 'that', 'Russian', 'and', 'Chinese', 'ha', '##', 'ck', 'ers', 'alleg', 'edly', 'plant', 'ed', 'ma', '##', 'l', '##', 'ware', 'on', 'the', 'site', ',', 'and', 'that', 'security', 'bre', 'ach', '##', 'es', 'occur', 'red', 'as', 'far', 'back', 'as', '2015', '.', '\"', 'An', 'invest', 'igation', 'by', 'ON', '##', 'R', '[', '[SEP]', '[SEP]']\n",
      "Tokenized Input IDs:  [50281, 50281, 47, 13706, 4238, 442, 21678, 28402, 874, 3251, 817, 14797, 817, 3423, 7110, 20394, 71, 967, 14775, 817, 1610, 817, 19, 13, 5388, 9, 5, 31543, 817, 76, 10, 1615, 783, 33577, 1542, 47, 13706, 5785, 1427, 9, 1139, 817, 51, 10, 1542, 71, 13454, 936, 324, 817, 1568, 936, 68, 817, 90, 817, 1653, 817, 886, 817, 5051, 1676, 2196, 395, 1065, 1076, 19579, 47745, 18480, 255, 16272, 1189, 12496, 10526, 13, 4064, 9638, 936, 18161, 817, 20, 15, 7130, 936, 783, 1139, 817, 51, 1136, 12943, 420, 13, 3251, 817, 14797, 817, 3423, 27337, 936, 25739, 953, 628, 37407, 68, 817, 90, 817, 1653, 817, 886, 817, 5051, 10075, 12862, 1615, 282, 3292, 34263, 87, 817, 335, 817, 1216, 817, 18754, 817, 77, 1005, 249, 953, 1433, 39098, 328, 817, 4904, 817, 85, 2147, 13, 23283, 839, 783, 47, 13706, 47219, 2246, 20356, 5785, 3339, 9755, 15, 8430, 2369, 15083, 80, 3535, 7110, 30714, 433, 13, 783, 20881, 1255, 265, 36330, 783, 28402, 874, 936, 4448, 661, 10328, 284, 16147, 297, 817, 1935, 13, 81, 817, 8701, 817, 2027, 13, 395, 33177, 2203, 18585, 13, 4609, 16534, 3431, 3787, 8656, 14, 73, 38364, 42316, 395, 29000, 615, 817, 681, 817, 2230, 817, 272, 1601, 15, 3251, 817, 14797, 817, 3423, 261, 531, 1171, 18913, 8, 84, 45242, 47745, 28402, 3191, 13, 46033, 249, 36, 817, 3561, 5182, 13, 19642, 15, 1147, 42241, 66, 32258, 14337, 249, 1342, 2977, 395, 21678, 25337, 4507, 11310, 13, 48590, 3062, 47745, 4238, 442, 249, 531, 12428, 14644, 1279, 977, 28402, 874, 10186, 4363, 15, 510, 11264, 261, 7821, 5336, 249, 250, 817, 12512, 817, 1173, 817, 272, 47745, 4238, 442, 13, 36667, 13, 395, 84, 817, 7675, 817, 69, 463, 4064, 1851, 1974, 81, 39320, 395, 9245, 817, 26185, 13, 296, 4263, 25337, 4507, 11310, 10328, 284, 81, 817, 40376, 817, 37583, 395, 321, 19808, 13, 1342, 2977, 1033, 290, 47745, 36667, 287, 1397, 13, 395, 250, 817, 8236, 817, 1076, 395, 615, 817, 681, 817, 2230, 817, 272, 47745, 28402, 3191, 15, 3251, 817, 14797, 817, 3423, 261, 66, 26717, 8522, 1542, 783, 19642, 8, 84, 47745, 4238, 442, 26454, 10394, 13, 601, 953, 1433, 39098, 13980, 261, 87, 1562, 936, 5358, 22768, 42316, 15, 8693, 2913, 13, 66, 22253, 1171, 24889, 304, 569, 1615, 510, 35854, 757, 14806, 3251, 817, 14797, 817, 3423, 8, 84, 68, 817, 90, 817, 1653, 817, 886, 817, 5051, 1288, 1224, 42959, 936, 34263, 40192, 22402, 13, 38198, 4052, 3529, 20987, 641, 10178, 36423, 10773, 936, 26717, 39098, 2811, 9328, 13, 35094, 977, 28579, 13, 16534, 12543, 26346, 14615, 265, 15, 28144, 13, 4714, 14, 4304, 87, 817, 335, 817, 1216, 817, 18754, 817, 77, 1005, 24767, 783, 28402, 874, 66, 817, 9458, 13, 19647, 783, 11264, 783, 23237, 1590, 3, 15271, 817, 615, 817, 20014, 817, 85, 3, 1615, 13174, 21107, 9088, 15, 1145, 5353, 262, 4064, 21258, 13980, 49053, 3404, 817, 375, 38198, 3256, 3529, 903, 314, 1976, 6, 1171, 3251, 817, 14797, 817, 3423, 8, 84, 2152, 735, 12796, 87, 335, 1216, 494, 936, 1595, 7305, 3113, 11714, 4303, 8076, 505, 16117, 585, 45997, 15, 510, 47745, 11264, 8, 84, 2211, 2392, 713, 7917, 4297, 7443, 249, 18948, 18161, 817, 21, 936, 14094, 33699, 936, 681, 2881, 3113, 15291, 1433, 13980, 1747, 3339, 13, 324, 15318, 14094, 33699, 15, 1139, 817, 51, 24889, 27285, 20513, 41071, 13, 395, 6050, 262, 37467, 3529, 3251, 817, 14797, 817, 3423, 27337, 936, 66, 817, 67, 504, 1615, 783, 68, 817, 90, 817, 1653, 817, 886, 817, 5051, 1676, 2196, 3529, 4524, 817, 9852, 783, 20936, 1171, 10328, 37813, 249, 783, 19642, 13, 262, 84, 698, 262, 14541, 2369, 22432, 3529, 783, 87, 817, 335, 817, 1216, 817, 18754, 817, 77, 1005, 12796, 282, 2394, 817, 69, 249, 1595, 7305, 15, 1552, 19657, 9346, 35065, 41071, 1615, 783, 7100, 3529, 27397, 395, 27223, 3227, 817, 777, 398, 38653, 15376, 8186, 264, 785, 817, 77, 817, 1935, 251, 783, 11264, 13, 395, 3529, 13980, 3381, 607, 817, 265, 30714, 433, 284, 14103, 2135, 284, 6620, 15, 3, 1145, 24889, 5538, 1615, 1139, 817, 51, 60, 50282, 50282]\n",
      "Word IDs Mapping:  [None, 0, 1, 1, 2, 2, 3, 4, 4, 5, 6, 6, 7, 7, 8, 9, 10, 10, 11, 12, 12, 13, 13, 14, 15, 16, 17, 18, 19, 19, 20, 21, 22, 23, 24, 25, 25, 26, 26, 27, 28, 29, 29, 30, 31, 32, 32, 33, 34, 35, 35, 36, 37, 38, 38, 39, 39, 40, 40, 41, 41, 42, 42, 43, 44, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 58, 59, 60, 61, 62, 63, 64, 64, 65, 65, 65, 66, 67, 68, 68, 69, 69, 70, 71, 72, 73, 74, 75, 76, 77, 77, 78, 78, 79, 79, 80, 80, 81, 81, 82, 83, 83, 84, 85, 86, 86, 87, 87, 88, 88, 89, 89, 89, 90, 91, 92, 93, 94, 95, 95, 96, 96, 96, 97, 98, 98, 99, 100, 100, 101, 101, 102, 103, 103, 104, 105, 106, 107, 108, 108, 108, 109, 110, 110, 111, 112, 113, 113, 113, 114, 115, 116, 116, 117, 118, 118, 119, 120, 121, 121, 122, 122, 123, 124, 125, 125, 126, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 135, 136, 137, 138, 138, 139, 140, 141, 142, 143, 143, 144, 144, 145, 145, 146, 147, 148, 149, 149, 150, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 159, 160, 161, 162, 163, 164, 164, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 174, 175, 176, 177, 177, 178, 179, 180, 181, 182, 183, 183, 184, 185, 186, 187, 188, 189, 190, 190, 191, 191, 192, 193, 194, 195, 196, 196, 197, 198, 199, 199, 200, 200, 201, 201, 202, 203, 203, 204, 205, 206, 207, 208, 209, 209, 210, 210, 210, 211, 212, 212, 213, 213, 214, 215, 216, 216, 217, 218, 218, 219, 219, 220, 221, 222, 223, 224, 224, 225, 225, 226, 227, 227, 228, 229, 229, 230, 230, 231, 232, 233, 233, 234, 235, 236, 237, 237, 238, 238, 239, 240, 241, 241, 242, 242, 243, 243, 244, 245, 245, 246, 247, 248, 248, 249, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 282, 282, 283, 284, 285, 285, 286, 287, 288, 288, 289, 289, 290, 291, 292, 293, 293, 294, 294, 295, 295, 296, 296, 297, 297, 298, 299, 300, 301, 302, 303, 304, 304, 305, 306, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 323, 324, 325, 326, 327, 328, 329, 330, 331, 331, 332, 332, 333, 333, 334, 334, 334, 335, 336, 337, 337, 338, 339, 339, 340, 341, 342, 343, 344, 345, 345, 346, 347, 348, 348, 349, 349, 350, 350, 351, 352, 353, 354, 355, 356, 357, 358, 358, 359, 360, 361, 362, 363, 364, 364, 365, 365, 366, 367, 367, 368, 369, 370, 371, 372, 372, 373, 373, 374, 375, 376, 376, 377, 378, 378, 378, 378, 379, 380, 380, 381, 382, 382, 383, 383, 383, 384, 384, 385, 386, 387, 388, 389, 390, 391, 391, 392, 392, 393, 393, 394, 395, 396, 397, 397, 398, 399, 400, 401, 402, 402, 403, 404, 405, 406, 407, 407, 408, 409, 409, 410, 411, 412, 413, 414, 414, 415, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 425, 426, 426, 427, 428, 429, 430, 430, 430, 431, 432, 433, 434, 434, 435, 435, 436, 436, 437, 437, 438, 438, 439, 440, 441, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 452, 453, 454, 455, 456, 457, 458, 459, 460, 460, 461, 461, 462, 462, 463, 463, 463, 464, 465, 465, 466, 466, 467, 468, 468, 469, 470, 471, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 482, 482, 483, 483, 484, 484, 485, 486, 486, 487, 487, 488, 489, 490, 491, 492, 493, 494, 495, 495, 496, 496, 497, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 506, 507, 508, 509, 509, 510, 511, None]\n",
      "Aligned Labels:  [-100, 14, 11, -100, 11, -100, 11, 11, -100, 11, 11, -100, 11, -100, 11, 11, 11, -100, 11, 11, -100, 11, -100, 11, 11, 11, 11, 11, 11, -100, 11, 11, 11, 11, 11, 11, -100, 11, -100, 11, 11, 11, -100, 11, 11, 11, -100, 11, 11, 11, -100, 11, 11, 11, -100, 11, -100, 11, -100, 11, -100, 11, -100, 11, 11, -100, 11, 11, 11, 11, 11, 16, 16, 16, 16, 16, 16, 16, 16, 16, -100, 16, 16, 16, 16, 10, 15, -100, 16, -100, -100, 16, 10, 15, -100, 15, -100, 16, 16, 16, 16, 16, 16, 16, 16, -100, 16, -100, 16, -100, 16, -100, 16, -100, 16, 16, -100, 16, 16, 16, -100, 16, -100, 16, -100, 16, -100, -100, 16, 16, 16, 16, 16, 16, -100, 16, -100, -100, 16, 16, -100, 16, 10, -100, 15, -100, 15, 15, -100, 16, 16, 16, 16, 16, -100, -100, 16, 16, -100, 16, 16, 16, -100, -100, 16, 16, 16, -100, 16, 16, -100, 16, 16, 16, -100, 16, -100, 16, 16, 16, -100, 16, -100, 16, 16, 16, 16, 16, 16, 16, 16, 16, -100, 16, 16, 16, -100, 16, 16, 16, 16, 16, -100, 16, -100, 16, -100, 16, 16, 10, 15, -100, 15, -100, 16, 16, 16, 6, 5, 5, 16, 16, 16, -100, 16, 16, 16, 6, 5, -100, -100, 5, 5, 16, 16, 16, 16, 16, 16, 16, 16, -100, 16, 16, 16, -100, 16, 16, 16, 16, 16, 16, -100, 16, 16, 16, 16, 16, 16, 16, -100, 16, -100, 16, 16, 16, 16, 16, -100, 16, 16, 16, -100, 16, -100, 16, -100, 16, 16, -100, 16, 16, 16, 16, 16, 16, -100, 16, -100, -100, 16, 16, -100, 16, -100, 16, 16, 16, -100, 16, 16, -100, 16, -100, 16, 16, 16, 16, 16, -100, 16, -100, 16, 16, -100, 16, 16, -100, 16, -100, 16, 16, 16, -100, 16, 16, 16, 16, -100, 16, -100, 16, 16, 16, -100, 16, -100, 16, -100, 16, 16, -100, 16, 10, 15, -100, 15, -100, 16, 16, 16, 16, 16, 16, 6, 5, 5, 16, 16, -100, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, -100, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, -100, -100, 16, 10, 15, -100, 16, 10, 15, -100, 15, -100, 15, 15, 16, 16, -100, 16, -100, 16, -100, 16, -100, 16, -100, 16, 16, 16, 16, 16, 16, 16, -100, 16, 16, -100, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, -100, 16, 16, 16, 16, 16, 16, 16, 16, -100, 16, -100, 16, -100, 16, -100, -100, 16, 16, 16, -100, 16, 16, -100, 16, 16, 16, 16, 16, 16, -100, 16, 16, 16, -100, 16, -100, 16, -100, 16, 16, 16, 16, 16, 16, 16, 16, -100, 16, 6, 16, 16, 10, 15, -100, 16, -100, 16, 16, -100, 16, 16, 16, 10, 15, -100, 15, -100, 15, 15, 16, -100, 16, 16, -100, -100, -100, 16, 16, -100, 16, 16, -100, 16, -100, -100, 16, -100, 16, 16, 16, 16, 16, 16, 16, -100, 16, -100, 16, -100, 16, 16, 16, 16, -100, 16, 16, 16, 16, 16, -100, 16, 16, 16, 16, 16, -100, 16, 16, -100, 16, 16, 16, 10, 15, -100, 16, -100, 16, 16, 16, 16, 16, 16, 16, 16, 10, 15, -100, 15, -100, 16, 16, 16, 16, -100, -100, 16, 16, 16, 16, -100, 16, -100, 16, -100, 16, -100, 16, -100, 16, 16, 16, -100, 16, 16, 16, 16, 16, 16, 16, 6, 16, 16, 16, -100, 16, 16, 16, 16, 16, 16, 16, 16, -100, 16, -100, 16, -100, 16, -100, -100, 16, 16, -100, 16, -100, 16, 16, -100, 16, 16, 16, -100, 16, 16, 16, 16, 16, 16, 6, 16, 6, 16, 16, -100, -100, 16, -100, 16, -100, 16, 16, -100, 16, -100, 16, 16, 16, 16, 16, 16, 16, 16, -100, 16, -100, 16, -100, 16, 16, 16, 16, 16, 16, 16, 16, 16, -100, 16, 10, 15, -100, 16, 14, -100]\n",
      "==========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process dataset with tokenizer and align labels\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        #padding=\"max_length\",\n",
    "        #max_length=512\n",
    "    )\n",
    "    labels = []\n",
    "    \n",
    "    for i, label in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Get mapping of subwords to words\n",
    "        previous_word_id = None\n",
    "        label_ids = []\n",
    "        \n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:  # Ignore special tokens (CLS, SEP, PAD)\n",
    "                label_ids.append(-100)\n",
    "            elif word_id != previous_word_id:  # Assign correct label only to first subword\n",
    "                label_ids.append(label[word_id])\n",
    "            else:\n",
    "                label_ids.append(-100)  # Assign -100 to subsequent subwords\n",
    "            \n",
    "            previous_word_id = word_id\n",
    "\n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    tokenized_inputs[\"decoded\"] = [[tokenizer.decode(i) for i in ids] for ids in tokenized_inputs.input_ids]\n",
    "\n",
    "    # ð¹ Debugging Step: Print a Sample to Verify Alignment\n",
    "    print(\"\\n==== Sample Debugging ====\")\n",
    "    print(\"Len examples: \", len(examples[\"tokens\"][-1]))\n",
    "    print(\"Len tokenized inputs: \", len(tokenized_inputs[\"input_ids\"][-1]))\n",
    "    print(\"Original Tokens: \", examples[\"tokens\"][-1])\n",
    "    print(\"decoded: \", tokenized_inputs[\"decoded\"][-1])\n",
    "    print(\"Tokenized Input IDs: \", tokenized_inputs[\"input_ids\"][-1])\n",
    "    print(\"Word IDs Mapping: \", word_ids)\n",
    "    print(\"Aligned Labels: \", labels[-1])\n",
    "    print(\"==========================\\n\")\n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "# Convert dataset to Hugging Face Dataset format\n",
    "dataset = Dataset.from_dict({\"tokens\": [entry[\"tokens\"] for entry in data], \"labels\": [entry[\"labels\"] for entry in data]})\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[27456], [4238, 442]], 'attention_mask': [[1], [1, 1]]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\" Nuclear\", \"waste\"], add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metric\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_labels = [[label for label in label_seq if label != -100] for label_seq in labels]\n",
    "    true_predictions = [[pred for pred, lab in zip(pred_seq, label_seq) if lab != -100] for pred_seq, label_seq in zip(predictions, labels)]\n",
    "    return seqeval.compute(predictions=true_predictions, references=true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",     # Output directory\n",
    "    eval_strategy=\"epoch\", # Evaluate after each epoch\n",
    "    save_strategy=\"epoch\",      # Save model after each epoch\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",       # Log directory\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,         # Save last 2 models only\n",
    "    fp16=True,                  # Enable mixed precision training for better performance\n",
    "    bf16=torch.cuda.is_bf16_supported(),  # Use BF16 if supported for better speed\n",
    "    optim=\"adamw_torch_fused\",  # Use fused optimizer for better CUDA performance\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magnu\\AppData\\Local\\Temp\\ipykernel_10116\\695041789.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided []",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ner_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2480\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2478\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2479\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[1;32m-> 2480\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2481\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[0;32m   2482\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:5153\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[1;34m(self, epoch_iterator, num_batches)\u001b[0m\n\u001b[0;32m   5151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[0;32m   5152\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 5153\u001b[0m         batch_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m   5154\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   5155\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\accelerate\\data_loader.py:563\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 563\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\data\\data_collator.py:45\u001b[0m, in \u001b[0;36mDataCollatorMixin.__call__\u001b[1;34m(self, features, return_tensors)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtf_call(features)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy_call(features)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\data\\data_collator.py:333\u001b[0m, in \u001b[0;36mDataCollatorForTokenClassification.torch_call\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    329\u001b[0m labels \u001b[38;5;241m=\u001b[39m [feature[label_name] \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features] \u001b[38;5;28;01mif\u001b[39;00m label_name \u001b[38;5;129;01min\u001b[39;00m features[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    331\u001b[0m no_labels_features \u001b[38;5;241m=\u001b[39m [{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m feature\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m label_name} \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[1;32m--> 333\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_labels_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\data\\data_collator.py:66\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[1;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 66\u001b[0m     padded \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m warning_state\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3305\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[1;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[0;32m   3303\u001b[0m \u001b[38;5;66;03m# The model's main input name, usually `input_ids`, has been passed for padding\u001b[39;00m\n\u001b[0;32m   3304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m encoded_inputs:\n\u001b[1;32m-> 3305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3307\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthat includes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but you provided \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(encoded_inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3308\u001b[0m     )\n\u001b[0;32m   3310\u001b[0m required_input \u001b[38;5;241m=\u001b[39m encoded_inputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m   3312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m required_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(required_input, Sized) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(required_input) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided []"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(\"./ner_model\")\n",
    "tokenizer.save_pretrained(\"./ner_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

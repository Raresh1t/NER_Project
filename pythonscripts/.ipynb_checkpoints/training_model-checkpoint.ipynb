{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flash-attn\n",
      "  Using cached flash_attn-2.7.4.post1.tar.gz (6.0 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\magnu\\\\AppData\\\\Local\\\\Temp\\\\pip-install-_h0lj359\\\\flash-attn_a64e364bab4b4bda827435c2541f476a\\\\csrc\\\\composable_kernel\\\\library\\\\include\\\\ck\\\\library\\\\tensor_operation_instance\\\\gpu\\\\grouped_conv_bwd_weight\\\\device_grouped_conv_bwd_weight_two_stage_xdl_instance.hpp'\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#! pip install seqeval\n",
    "#! pip install evaluate\n",
    "#! pip install pandas\n",
    "#! pip install datasets\n",
    "#! pip install torch\n",
    "#! pip install transformers\n",
    "#! pip install scikit-learn\n",
    "#! pip install ninja\n",
    "#! pip install flash-attn\n",
    "#! pip install packaging\n",
    "#! pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import evaluate\n",
    "import seqeval\n",
    "import accelerate\n",
    "import transformers\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from flash_attn.flash_attention import FlashAttention\n",
    "# Flash Attention for faster training\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # Enable TF32 for better performance\n",
    "torch.backends.cudnn.benchmark = True  # Enable CuDNN auto-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "DATA_PATH = \"tokenized_ner_data_6.json\"\n",
    "with open(DATA_PATH, \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_list(data)\n",
    "testing_dataset = Dataset.from_list(data)\n",
    "train_dataset, eval_dataset = dataset.train_test_split(test_size=0.2).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['tokens', 'labels'],\n",
      "    num_rows: 61\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "#print(train_dataset)\n",
    "#print(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-CVE': 0, 'B-Event': 1, 'B-LOC': 2, 'B-MAL-ORG': 3, 'B-MISC': 4, 'B-Malware': 5, 'B-ORG': 6, 'B-PER': 7, 'B-Software': 8, 'I-CVE': 9, 'I-Event': 10, 'I-LOC': 11, 'I-MAL-ORG': 12, 'I-MISC': 13, 'I-Malware': 14, 'I-ORG': 15, 'I-PER': 16, 'I-Software': 17, 'O': 18}\n"
     ]
    }
   ],
   "source": [
    "unique_labels = set(label for entry in dataset for label in entry[\"labels\"])\n",
    "label2id = {label: i for i, label in enumerate(sorted(unique_labels))}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(label2id)  # Mapping of labels to IDs\n",
    "\n",
    "# Convert text labels to integer IDs\n",
    "#data[\"labels\"] = [label2id[label] for label in data[\"labels\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "MODEL_NAME = \"google-bert/bert-base-cased\"\n",
    "#\"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "#model = AutoModelForTokenClassification.from_pretrained(\n",
    "#    MODEL_NAME,\n",
    "#    num_labels=len(label2id),\n",
    "#    id2label=id2label,\n",
    "#    label2id=label2id\n",
    "#)\n",
    "\n",
    "# Enable Flash Attention in the model (if applicable)\n",
    "#model.config.use_flash_attention = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3165df60e3b84a33be84b5fb1cdf12a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/61 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_tokens_and_labels(data):\n",
    "    \"\"\"Convert tokenized text into IDs and align labels.\"\"\"\n",
    "    data[\"input_ids\"] = tokenizer.convert_tokens_to_ids(data[\"tokens\"])\n",
    "    data[\"attention_mask\"] = [1] * len(data[\"input_ids\"])  # Mask for all tokens\n",
    "    \n",
    "    # Infer word_ids manually\n",
    "    word_ids = []\n",
    "    word_idx = -1 # Initialize\n",
    "    for token in data[\"tokens\"]:\n",
    "        if token in [\"[CLS]\", \"[SEP]\"]:\n",
    "            word_ids.append(None)\n",
    "        elif token.startswith(\"##\"):\n",
    "            word_ids.append(word_idx)\n",
    "        else:\n",
    "            word_idx += 1\n",
    "            word_ids.append(word_idx)\n",
    "\n",
    "    data[\"word_ids\"] = word_ids\n",
    "    \n",
    "    # Convert text labels to integer IDs\n",
    "    data[\"labels\"] = [label2id[label] for label in data[\"labels\"]]\n",
    "\n",
    "    return data\n",
    "\n",
    "# Apply transformation to dataset\n",
    "dataset = dataset.map(process_tokens_and_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', 'A', 'new', 'ransom', '##ware', '-', 'as', '-', 'a', '-', 'service', '(', 'Ra', '##a', '##S', ')', 'operation', 'named', 'C', '##ica', '##da', '##33', '##01', 'has', 'already', 'listed', '19', 'victims', 'on', 'its', 'ex', '##tor', '##tion', 'portal', ',', 'as', 'it', 'quickly', 'attacked', 'companies', 'worldwide', '.', 'The', 'new', 'c', '##y', '##ber', '##c', '##rim', '##e', 'operation', 'is', 'named', 'after', 'the', 'mysterious', '2012', '-', '2014', 'online', '/', 'real', '-', 'world', 'game', 'that', 'involved', 'elaborate', 'cry', '##pt', '##ographic', 'puzzles', 'and', 'used', 'the', 'same', 'logo', 'for', 'promotion', 'on', 'c', '##y', '##ber', '##c', '##rim', '##e', 'forums', '.', 'However', ',', 'there', \"'\", 's', 'no', 'connection', 'between', 'the', 'two', ',', 'and', 'the', 'legitimate', 'project', 'has', 'issued', 'a', 'statement', 'to', 're', '##nounce', 'any', 'association', 'and', 'con', '##de', '##m', '##n', 'the', 'ransom', '##ware', 'operators', \"'\", 'actions', '.', 'The', 'C', '##ica', '##da', '##33', '##01', 'Ra', '##a', '##S', 'first', 'began', 'promoting', 'the', 'operation', 'and', 'recruiting', 'affiliates', 'on', 'June', '29', ',', '202', '##4', ',', 'in', 'a', 'forum', 'post', 'to', 'the', 'ransom', '##ware', 'and', 'c', '##y', '##ber', '##c', '##rim', '##e', 'forum', 'known', 'as', 'RAM', '##P', '.', 'However', ',', 'B', '##lee', '##ping', '##C', '##om', '##pute', '##r', 'is', 'aware', 'of', 'C', '##ica', '##da', 'attacks', 'as', 'early', 'as', 'June', '6', ',', 'indicating', 'that', 'the', 'gang', 'was', 'operating', 'independently', 'before', 'attempting', 'to', 'recruit', 'affiliates', '.', 'Like', 'other', 'ransom', '##ware', 'operations', ',', 'C', '##ica', '##da', '##33', '##01', 'conducts', 'double', '-', 'ex', '##tor', '##tion', 'tactics', 'where', 'they', 'breach', 'corporate', 'networks', ',', 'steal', 'data', ',', 'and', 'then', 'en', '##c', '##ry', '##pt', 'devices', '.', 'The', 'encryption', 'key', 'and', 'threats', 'to', 'leak', 'stolen', 'data', 'are', 'then', 'used', 'as', 'leverage', 'to', 'scare', 'victims', 'into', 'paying', 'a', 'ransom', '.', 'The', 'threat', 'actors', 'operate', 'a', 'data', 'leak', 'site', 'that', 'is', 'used', 'as', 'part', 'of', 'their', 'double', '-', 'ex', '##tor', '##tion', 'scheme', '.', 'An', 'analysis', 'of', 'the', 'new', 'ma', '##l', '##ware', 'by', 'True', '##se', '##c', 'revealed', 'significant', 'overlap', '##s', 'between', 'C', '##ica', '##da', '##33', '##01', 'and', 'AL', '##P', '##H', '##V', '/', 'Black', '##C', '##at', ',', 'indicating', 'a', 'possible', 're', '##brand', 'or', 'a', 'fork', 'created', 'by', 'former', 'AL', '##P', '##H', '##V', \"'\", 's', 'core', 'team', 'members', '.', 'This', 'is', 'based', 'on', 'the', 'fact', 'that', ':', 'For', 'context', ',', 'AL', '##P', '##H', '##V', 'performed', 'an', 'exit', 's', '##cam', 'in', 'early', 'March', '202', '##4', 'involving', 'fake', 'claims', 'about', 'an', 'FBI', 'take', '##down', 'operation', 'after', 'they', 'stole', 'a', 'massive', '$', '22', 'million', 'payment', 'from', 'Change', 'Healthcare', 'from', 'one', 'of', 'their', 'affiliates', '.', 'True', '##se', '##c', 'has', 'also', 'found', 'indication', '##s', 'that', 'the', 'C', '##ica', '##da', '##33', '##01', 'ransom', '##ware', 'operation', 'may', 'partner', 'with', 'or', 'utilize', 'the', 'B', '##ru', '##tus', 'b', '##ot', '##net', 'for', 'initial', 'access', 'to', 'corporate', 'networks', '.', 'That', 'b', '##ot', '##net', 'was', 'previously', 'associated', 'with', 'global', '-', 'scale', 'VP', '##N', 'br', '##ute', '-', 'forcing', 'activities', 'targeting', 'C', '##isco', ',', 'Fort', '##inet', ',', 'Pa', '##lo', 'Alto', ',', 'and', 'Sonic', '##W', '##all', 'appliances', '.', 'It', \"'\", 's', 'worth', 'noting', 'that', 'the', 'B', '##ru', '##tus', 'activity', 'was', 'first', 'spotted', 'two', 'weeks', 'after', 'AL', '##P', '##H', '##V', 'shut', 'down', 'operations', ',', 'so', 'the', 'link', 'between', 'the', 'two', 'groups', 'still', 'stands', 'in', 'terms', 'of', 'timeline', '##s', '.', 'C', '##ica', '##da', '##33', '##01', 'is', 'a', 'R', '##ust', '-', 'based', '[SEP]'], 'labels': [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 3, 12, 12, 12, 12, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 3, 12, 12, 12, 12, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 6, 15, 15, 15, 15, 15, 15, 18, 18, 18, 3, 12, 12, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 3, 12, 12, 12, 12, 1, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 6, 15, 15, 18, 18, 18, 18, 18, 3, 12, 12, 12, 12, 18, 3, 12, 12, 12, 18, 3, 12, 12, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 3, 12, 12, 12, 12, 12, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 3, 12, 12, 12, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 6, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 6, 15, 18, 18, 18, 18, 18, 18, 6, 15, 15, 18, 18, 18, 18, 18, 18, 18, 3, 12, 12, 12, 12, 18, 18, 18, 18, 18, 18, 18, 18, 18, 5, 14, 14, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 6, 15, 18, 6, 15, 18, 6, 15, 15, 18, 18, 6, 15, 15, 18, 18, 18, 18, 18, 18, 18, 18, 18, 3, 12, 12, 18, 18, 18, 18, 18, 18, 18, 6, 15, 15, 15, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 3, 12, 12, 12, 12, 18, 18, 18, 18, 18, 18, 18], 'input_ids': [101, 138, 1207, 25057, 7109, 118, 1112, 118, 170, 118, 1555, 113, 16890, 1161, 1708, 114, 2805, 1417, 140, 4578, 1810, 23493, 24400, 1144, 1640, 2345, 1627, 5256, 1113, 1157, 4252, 2772, 2116, 10823, 117, 1112, 1122, 1976, 3623, 2557, 4529, 119, 1109, 1207, 172, 1183, 3169, 1665, 10205, 1162, 2805, 1110, 1417, 1170, 1103, 8198, 1368, 118, 1387, 3294, 120, 1842, 118, 1362, 1342, 1115, 2017, 9427, 5354, 6451, 9597, 22747, 1105, 1215, 1103, 1269, 7998, 1111, 4166, 1113, 172, 1183, 3169, 1665, 10205, 1162, 25438, 119, 1438, 117, 1175, 112, 188, 1185, 3797, 1206, 1103, 1160, 117, 1105, 1103, 11582, 1933, 1144, 3010, 170, 4195, 1106, 1231, 25196, 1251, 3852, 1105, 14255, 2007, 1306, 1179, 1103, 25057, 7109, 9298, 112, 3721, 119, 1109, 140, 4578, 1810, 23493, 24400, 16890, 1161, 1708, 1148, 1310, 7495, 1103, 2805, 1105, 16226, 20795, 1113, 1340, 1853, 117, 17881, 1527, 117, 1107, 170, 13912, 2112, 1106, 1103, 25057, 7109, 1105, 172, 1183, 3169, 1665, 10205, 1162, 13912, 1227, 1112, 20898, 2101, 119, 1438, 117, 139, 6894, 2624, 1658, 4165, 22662, 1197, 1110, 4484, 1104, 140, 4578, 1810, 3690, 1112, 1346, 1112, 1340, 127, 117, 7713, 1115, 1103, 6939, 1108, 3389, 8942, 1196, 6713, 1106, 14240, 20795, 119, 2409, 1168, 25057, 7109, 2500, 117, 140, 4578, 1810, 23493, 24400, 19706, 2702, 118, 4252, 2772, 2116, 10524, 1187, 1152, 13275, 6214, 6379, 117, 8991, 2233, 117, 1105, 1173, 4035, 1665, 1616, 6451, 5197, 119, 1109, 26463, 2501, 1105, 8657, 1106, 19299, 7251, 2233, 1132, 1173, 1215, 1112, 24228, 1106, 13671, 5256, 1154, 6573, 170, 25057, 119, 1109, 4433, 5681, 4732, 170, 2233, 19299, 1751, 1115, 1110, 1215, 1112, 1226, 1104, 1147, 2702, 118, 4252, 2772, 2116, 5471, 119, 1760, 3622, 1104, 1103, 1207, 12477, 1233, 7109, 1118, 7817, 2217, 1665, 3090, 2418, 19235, 1116, 1206, 140, 4578, 1810, 23493, 24400, 1105, 18589, 2101, 3048, 2559, 120, 2117, 1658, 2980, 117, 7713, 170, 1936, 1231, 25123, 1137, 170, 13097, 1687, 1118, 1393, 18589, 2101, 3048, 2559, 112, 188, 4160, 1264, 1484, 119, 1188, 1110, 1359, 1113, 1103, 1864, 1115, 131, 1370, 5618, 117, 18589, 2101, 3048, 2559, 1982, 1126, 6300, 188, 24282, 1107, 1346, 1345, 17881, 1527, 5336, 8406, 3711, 1164, 1126, 8099, 1321, 5455, 2805, 1170, 1152, 10566, 170, 4672, 109, 1659, 1550, 7727, 1121, 9091, 22993, 1121, 1141, 1104, 1147, 20795, 119, 7817, 2217, 1665, 1144, 1145, 1276, 12754, 1116, 1115, 1103, 140, 4578, 1810, 23493, 24400, 25057, 7109, 2805, 1336, 3547, 1114, 1137, 17573, 1103, 139, 5082, 4814, 171, 3329, 6097, 1111, 3288, 2469, 1106, 6214, 6379, 119, 1337, 171, 3329, 6097, 1108, 2331, 2628, 1114, 4265, 118, 3418, 23659, 2249, 9304, 6140, 118, 6524, 2619, 15141, 140, 21097, 117, 3144, 27411, 117, 19585, 2858, 17762, 117, 1105, 16202, 2924, 5727, 26498, 119, 1135, 112, 188, 3869, 9095, 1115, 1103, 139, 5082, 4814, 3246, 1108, 1148, 6910, 1160, 2277, 1170, 18589, 2101, 3048, 2559, 3210, 1205, 2500, 117, 1177, 1103, 5088, 1206, 1103, 1160, 2114, 1253, 4061, 1107, 2538, 1104, 21169, 1116, 119, 140, 4578, 1810, 23493, 24400, 1110, 170, 155, 8954, 118, 1359, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'word_ids': [None, 0, 1, 2, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 11, 12, 13, 14, 14, 14, 14, 14, 15, 16, 17, 18, 19, 20, 21, 22, 22, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 34, 34, 34, 34, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 53, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 63, 63, 63, 63, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 86, 87, 88, 89, 90, 90, 90, 90, 91, 92, 92, 93, 94, 95, 96, 97, 98, 98, 98, 98, 98, 99, 99, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 112, 113, 114, 115, 116, 117, 118, 119, 120, 120, 121, 122, 122, 122, 122, 122, 122, 123, 124, 125, 126, 126, 127, 128, 129, 130, 130, 130, 130, 130, 130, 130, 131, 132, 133, 134, 134, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 157, 158, 159, 160, 160, 160, 160, 160, 161, 162, 163, 164, 164, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 177, 177, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 219, 219, 220, 221, 222, 223, 224, 225, 226, 227, 227, 227, 228, 229, 229, 229, 230, 231, 232, 232, 233, 234, 234, 234, 234, 234, 235, 236, 236, 236, 236, 237, 238, 238, 238, 239, 240, 241, 242, 243, 243, 244, 245, 246, 247, 248, 249, 250, 250, 250, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 268, 268, 268, 269, 270, 271, 272, 272, 273, 274, 275, 276, 276, 277, 278, 279, 280, 281, 282, 283, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 303, 303, 304, 305, 306, 307, 307, 308, 309, 310, 310, 310, 310, 310, 311, 311, 312, 313, 314, 315, 316, 317, 318, 319, 319, 319, 320, 320, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 329, 329, 330, 331, 332, 333, 334, 335, 336, 337, 337, 338, 338, 339, 340, 341, 342, 343, 343, 344, 345, 345, 346, 347, 347, 348, 349, 350, 351, 351, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 361, 361, 362, 363, 364, 365, 366, 367, 368, 369, 369, 369, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 386, 387, 388, 388, 388, 388, 388, 389, 390, 391, 391, 392, 393, None]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0: ID None\n",
      "Index 1: ID 0\n",
      "Index 2: ID 1\n",
      "Index 3: ID 2\n",
      "Index 4: ID 2\n",
      "Index 5: ID 3\n",
      "Index 6: ID 4\n",
      "Index 7: ID 5\n",
      "Index 8: ID 6\n",
      "Index 9: ID 7\n",
      "Index 10: ID 8\n",
      "Index 11: ID 9\n",
      "Index 12: ID 10\n",
      "Index 13: ID 10\n",
      "Index 14: ID 10\n",
      "Index 15: ID 11\n",
      "Index 16: ID 12\n",
      "Index 17: ID 13\n",
      "Index 18: ID 14\n",
      "Index 19: ID 14\n",
      "Index 20: ID 14\n",
      "Index 21: ID 14\n",
      "Index 22: ID 14\n",
      "Index 23: ID 15\n",
      "Index 24: ID 16\n",
      "Index 25: ID 17\n",
      "Index 26: ID 18\n",
      "Index 27: ID 19\n",
      "Index 28: ID 20\n",
      "Index 29: ID 21\n",
      "Index 30: ID 22\n",
      "Index 31: ID 22\n",
      "Index 32: ID 22\n",
      "Index 33: ID 23\n",
      "Index 34: ID 24\n",
      "Index 35: ID 25\n",
      "Index 36: ID 26\n",
      "Index 37: ID 27\n",
      "Index 38: ID 28\n",
      "Index 39: ID 29\n",
      "Index 40: ID 30\n",
      "Index 41: ID 31\n",
      "Index 42: ID 32\n",
      "Index 43: ID 33\n",
      "Index 44: ID 34\n",
      "Index 45: ID 34\n",
      "Index 46: ID 34\n",
      "Index 47: ID 34\n",
      "Index 48: ID 34\n",
      "Index 49: ID 34\n",
      "Index 50: ID 35\n",
      "Index 51: ID 36\n",
      "Index 52: ID 37\n",
      "Index 53: ID 38\n",
      "Index 54: ID 39\n",
      "Index 55: ID 40\n",
      "Index 56: ID 41\n",
      "Index 57: ID 42\n",
      "Index 58: ID 43\n",
      "Index 59: ID 44\n",
      "Index 60: ID 45\n",
      "Index 61: ID 46\n",
      "Index 62: ID 47\n",
      "Index 63: ID 48\n",
      "Index 64: ID 49\n",
      "Index 65: ID 50\n",
      "Index 66: ID 51\n",
      "Index 67: ID 52\n",
      "Index 68: ID 53\n",
      "Index 69: ID 53\n",
      "Index 70: ID 53\n",
      "Index 71: ID 54\n",
      "Index 72: ID 55\n",
      "Index 73: ID 56\n",
      "Index 74: ID 57\n",
      "Index 75: ID 58\n",
      "Index 76: ID 59\n",
      "Index 77: ID 60\n",
      "Index 78: ID 61\n",
      "Index 79: ID 62\n",
      "Index 80: ID 63\n",
      "Index 81: ID 63\n",
      "Index 82: ID 63\n",
      "Index 83: ID 63\n",
      "Index 84: ID 63\n",
      "Index 85: ID 63\n",
      "Index 86: ID 64\n",
      "Index 87: ID 65\n",
      "Index 88: ID 66\n",
      "Index 89: ID 67\n",
      "Index 90: ID 68\n",
      "Index 91: ID 69\n",
      "Index 92: ID 70\n",
      "Index 93: ID 71\n",
      "Index 94: ID 72\n",
      "Index 95: ID 73\n",
      "Index 96: ID 74\n",
      "Index 97: ID 75\n",
      "Index 98: ID 76\n",
      "Index 99: ID 77\n",
      "Index 100: ID 78\n",
      "Index 101: ID 79\n",
      "Index 102: ID 80\n",
      "Index 103: ID 81\n",
      "Index 104: ID 82\n",
      "Index 105: ID 83\n",
      "Index 106: ID 84\n",
      "Index 107: ID 85\n",
      "Index 108: ID 86\n",
      "Index 109: ID 86\n",
      "Index 110: ID 87\n",
      "Index 111: ID 88\n",
      "Index 112: ID 89\n",
      "Index 113: ID 90\n",
      "Index 114: ID 90\n",
      "Index 115: ID 90\n",
      "Index 116: ID 90\n",
      "Index 117: ID 91\n",
      "Index 118: ID 92\n",
      "Index 119: ID 92\n",
      "Index 120: ID 93\n",
      "Index 121: ID 94\n",
      "Index 122: ID 95\n",
      "Index 123: ID 96\n",
      "Index 124: ID 97\n",
      "Index 125: ID 98\n",
      "Index 126: ID 98\n",
      "Index 127: ID 98\n",
      "Index 128: ID 98\n",
      "Index 129: ID 98\n",
      "Index 130: ID 99\n",
      "Index 131: ID 99\n",
      "Index 132: ID 99\n",
      "Index 133: ID 100\n",
      "Index 134: ID 101\n",
      "Index 135: ID 102\n",
      "Index 136: ID 103\n",
      "Index 137: ID 104\n",
      "Index 138: ID 105\n",
      "Index 139: ID 106\n",
      "Index 140: ID 107\n",
      "Index 141: ID 108\n",
      "Index 142: ID 109\n",
      "Index 143: ID 110\n",
      "Index 144: ID 111\n",
      "Index 145: ID 112\n",
      "Index 146: ID 112\n",
      "Index 147: ID 113\n",
      "Index 148: ID 114\n",
      "Index 149: ID 115\n",
      "Index 150: ID 116\n",
      "Index 151: ID 117\n",
      "Index 152: ID 118\n",
      "Index 153: ID 119\n",
      "Index 154: ID 120\n",
      "Index 155: ID 120\n",
      "Index 156: ID 121\n",
      "Index 157: ID 122\n",
      "Index 158: ID 122\n",
      "Index 159: ID 122\n",
      "Index 160: ID 122\n",
      "Index 161: ID 122\n",
      "Index 162: ID 122\n",
      "Index 163: ID 123\n",
      "Index 164: ID 124\n",
      "Index 165: ID 125\n",
      "Index 166: ID 126\n",
      "Index 167: ID 126\n",
      "Index 168: ID 127\n",
      "Index 169: ID 128\n",
      "Index 170: ID 129\n",
      "Index 171: ID 130\n",
      "Index 172: ID 130\n",
      "Index 173: ID 130\n",
      "Index 174: ID 130\n",
      "Index 175: ID 130\n",
      "Index 176: ID 130\n",
      "Index 177: ID 130\n",
      "Index 178: ID 131\n",
      "Index 179: ID 132\n",
      "Index 180: ID 133\n",
      "Index 181: ID 134\n",
      "Index 182: ID 134\n",
      "Index 183: ID 134\n",
      "Index 184: ID 135\n",
      "Index 185: ID 136\n",
      "Index 186: ID 137\n",
      "Index 187: ID 138\n",
      "Index 188: ID 139\n",
      "Index 189: ID 140\n",
      "Index 190: ID 141\n",
      "Index 191: ID 142\n",
      "Index 192: ID 143\n",
      "Index 193: ID 144\n",
      "Index 194: ID 145\n",
      "Index 195: ID 146\n",
      "Index 196: ID 147\n",
      "Index 197: ID 148\n",
      "Index 198: ID 149\n",
      "Index 199: ID 150\n",
      "Index 200: ID 151\n",
      "Index 201: ID 152\n",
      "Index 202: ID 153\n",
      "Index 203: ID 154\n",
      "Index 204: ID 155\n",
      "Index 205: ID 156\n",
      "Index 206: ID 157\n",
      "Index 207: ID 157\n",
      "Index 208: ID 158\n",
      "Index 209: ID 159\n",
      "Index 210: ID 160\n",
      "Index 211: ID 160\n",
      "Index 212: ID 160\n",
      "Index 213: ID 160\n",
      "Index 214: ID 160\n",
      "Index 215: ID 161\n",
      "Index 216: ID 162\n",
      "Index 217: ID 163\n",
      "Index 218: ID 164\n",
      "Index 219: ID 164\n",
      "Index 220: ID 164\n",
      "Index 221: ID 165\n",
      "Index 222: ID 166\n",
      "Index 223: ID 167\n",
      "Index 224: ID 168\n",
      "Index 225: ID 169\n",
      "Index 226: ID 170\n",
      "Index 227: ID 171\n",
      "Index 228: ID 172\n",
      "Index 229: ID 173\n",
      "Index 230: ID 174\n",
      "Index 231: ID 175\n",
      "Index 232: ID 176\n",
      "Index 233: ID 177\n",
      "Index 234: ID 177\n",
      "Index 235: ID 177\n",
      "Index 236: ID 177\n",
      "Index 237: ID 178\n",
      "Index 238: ID 179\n",
      "Index 239: ID 180\n",
      "Index 240: ID 181\n",
      "Index 241: ID 182\n",
      "Index 242: ID 183\n",
      "Index 243: ID 184\n",
      "Index 244: ID 185\n",
      "Index 245: ID 186\n",
      "Index 246: ID 187\n",
      "Index 247: ID 188\n",
      "Index 248: ID 189\n",
      "Index 249: ID 190\n",
      "Index 250: ID 191\n",
      "Index 251: ID 192\n",
      "Index 252: ID 193\n",
      "Index 253: ID 194\n",
      "Index 254: ID 195\n",
      "Index 255: ID 196\n",
      "Index 256: ID 197\n",
      "Index 257: ID 198\n",
      "Index 258: ID 199\n",
      "Index 259: ID 200\n",
      "Index 260: ID 201\n",
      "Index 261: ID 202\n",
      "Index 262: ID 203\n",
      "Index 263: ID 204\n",
      "Index 264: ID 205\n",
      "Index 265: ID 206\n",
      "Index 266: ID 207\n",
      "Index 267: ID 208\n",
      "Index 268: ID 209\n",
      "Index 269: ID 210\n",
      "Index 270: ID 211\n",
      "Index 271: ID 212\n",
      "Index 272: ID 213\n",
      "Index 273: ID 214\n",
      "Index 274: ID 215\n",
      "Index 275: ID 216\n",
      "Index 276: ID 217\n",
      "Index 277: ID 218\n",
      "Index 278: ID 219\n",
      "Index 279: ID 219\n",
      "Index 280: ID 219\n",
      "Index 281: ID 220\n",
      "Index 282: ID 221\n",
      "Index 283: ID 222\n",
      "Index 284: ID 223\n",
      "Index 285: ID 224\n",
      "Index 286: ID 225\n",
      "Index 287: ID 226\n",
      "Index 288: ID 227\n",
      "Index 289: ID 227\n",
      "Index 290: ID 227\n",
      "Index 291: ID 228\n",
      "Index 292: ID 229\n",
      "Index 293: ID 229\n",
      "Index 294: ID 229\n",
      "Index 295: ID 230\n",
      "Index 296: ID 231\n",
      "Index 297: ID 232\n",
      "Index 298: ID 232\n",
      "Index 299: ID 233\n",
      "Index 300: ID 234\n",
      "Index 301: ID 234\n",
      "Index 302: ID 234\n",
      "Index 303: ID 234\n",
      "Index 304: ID 234\n",
      "Index 305: ID 235\n",
      "Index 306: ID 236\n",
      "Index 307: ID 236\n",
      "Index 308: ID 236\n",
      "Index 309: ID 236\n",
      "Index 310: ID 237\n",
      "Index 311: ID 238\n",
      "Index 312: ID 238\n",
      "Index 313: ID 238\n",
      "Index 314: ID 239\n",
      "Index 315: ID 240\n",
      "Index 316: ID 241\n",
      "Index 317: ID 242\n",
      "Index 318: ID 243\n",
      "Index 319: ID 243\n",
      "Index 320: ID 244\n",
      "Index 321: ID 245\n",
      "Index 322: ID 246\n",
      "Index 323: ID 247\n",
      "Index 324: ID 248\n",
      "Index 325: ID 249\n",
      "Index 326: ID 250\n",
      "Index 327: ID 250\n",
      "Index 328: ID 250\n",
      "Index 329: ID 250\n",
      "Index 330: ID 251\n",
      "Index 331: ID 252\n",
      "Index 332: ID 253\n",
      "Index 333: ID 254\n",
      "Index 334: ID 255\n",
      "Index 335: ID 256\n",
      "Index 336: ID 257\n",
      "Index 337: ID 258\n",
      "Index 338: ID 259\n",
      "Index 339: ID 260\n",
      "Index 340: ID 261\n",
      "Index 341: ID 262\n",
      "Index 342: ID 263\n",
      "Index 343: ID 264\n",
      "Index 344: ID 265\n",
      "Index 345: ID 266\n",
      "Index 346: ID 267\n",
      "Index 347: ID 268\n",
      "Index 348: ID 268\n",
      "Index 349: ID 268\n",
      "Index 350: ID 268\n",
      "Index 351: ID 269\n",
      "Index 352: ID 270\n",
      "Index 353: ID 271\n",
      "Index 354: ID 272\n",
      "Index 355: ID 272\n",
      "Index 356: ID 273\n",
      "Index 357: ID 274\n",
      "Index 358: ID 275\n",
      "Index 359: ID 276\n",
      "Index 360: ID 276\n",
      "Index 361: ID 277\n",
      "Index 362: ID 278\n",
      "Index 363: ID 279\n",
      "Index 364: ID 280\n",
      "Index 365: ID 281\n",
      "Index 366: ID 282\n",
      "Index 367: ID 283\n",
      "Index 368: ID 283\n",
      "Index 369: ID 284\n",
      "Index 370: ID 285\n",
      "Index 371: ID 286\n",
      "Index 372: ID 287\n",
      "Index 373: ID 288\n",
      "Index 374: ID 289\n",
      "Index 375: ID 290\n",
      "Index 376: ID 291\n",
      "Index 377: ID 292\n",
      "Index 378: ID 293\n",
      "Index 379: ID 294\n",
      "Index 380: ID 295\n",
      "Index 381: ID 296\n",
      "Index 382: ID 297\n",
      "Index 383: ID 298\n",
      "Index 384: ID 299\n",
      "Index 385: ID 300\n",
      "Index 386: ID 301\n",
      "Index 387: ID 302\n",
      "Index 388: ID 303\n",
      "Index 389: ID 303\n",
      "Index 390: ID 303\n",
      "Index 391: ID 304\n",
      "Index 392: ID 305\n",
      "Index 393: ID 306\n",
      "Index 394: ID 307\n",
      "Index 395: ID 307\n",
      "Index 396: ID 308\n",
      "Index 397: ID 309\n",
      "Index 398: ID 310\n",
      "Index 399: ID 310\n",
      "Index 400: ID 310\n",
      "Index 401: ID 310\n",
      "Index 402: ID 310\n",
      "Index 403: ID 311\n",
      "Index 404: ID 311\n",
      "Index 405: ID 312\n",
      "Index 406: ID 313\n",
      "Index 407: ID 314\n",
      "Index 408: ID 315\n",
      "Index 409: ID 316\n",
      "Index 410: ID 317\n",
      "Index 411: ID 318\n",
      "Index 412: ID 319\n",
      "Index 413: ID 319\n",
      "Index 414: ID 319\n",
      "Index 415: ID 320\n",
      "Index 416: ID 320\n",
      "Index 417: ID 320\n",
      "Index 418: ID 321\n",
      "Index 419: ID 322\n",
      "Index 420: ID 323\n",
      "Index 421: ID 324\n",
      "Index 422: ID 325\n",
      "Index 423: ID 326\n",
      "Index 424: ID 327\n",
      "Index 425: ID 328\n",
      "Index 426: ID 329\n",
      "Index 427: ID 329\n",
      "Index 428: ID 329\n",
      "Index 429: ID 330\n",
      "Index 430: ID 331\n",
      "Index 431: ID 332\n",
      "Index 432: ID 333\n",
      "Index 433: ID 334\n",
      "Index 434: ID 335\n",
      "Index 435: ID 336\n",
      "Index 436: ID 337\n",
      "Index 437: ID 337\n",
      "Index 438: ID 338\n",
      "Index 439: ID 338\n",
      "Index 440: ID 339\n",
      "Index 441: ID 340\n",
      "Index 442: ID 341\n",
      "Index 443: ID 342\n",
      "Index 444: ID 343\n",
      "Index 445: ID 343\n",
      "Index 446: ID 344\n",
      "Index 447: ID 345\n",
      "Index 448: ID 345\n",
      "Index 449: ID 346\n",
      "Index 450: ID 347\n",
      "Index 451: ID 347\n",
      "Index 452: ID 348\n",
      "Index 453: ID 349\n",
      "Index 454: ID 350\n",
      "Index 455: ID 351\n",
      "Index 456: ID 351\n",
      "Index 457: ID 351\n",
      "Index 458: ID 352\n",
      "Index 459: ID 353\n",
      "Index 460: ID 354\n",
      "Index 461: ID 355\n",
      "Index 462: ID 356\n",
      "Index 463: ID 357\n",
      "Index 464: ID 358\n",
      "Index 465: ID 359\n",
      "Index 466: ID 360\n",
      "Index 467: ID 361\n",
      "Index 468: ID 361\n",
      "Index 469: ID 361\n",
      "Index 470: ID 362\n",
      "Index 471: ID 363\n",
      "Index 472: ID 364\n",
      "Index 473: ID 365\n",
      "Index 474: ID 366\n",
      "Index 475: ID 367\n",
      "Index 476: ID 368\n",
      "Index 477: ID 369\n",
      "Index 478: ID 369\n",
      "Index 479: ID 369\n",
      "Index 480: ID 369\n",
      "Index 481: ID 370\n",
      "Index 482: ID 371\n",
      "Index 483: ID 372\n",
      "Index 484: ID 373\n",
      "Index 485: ID 374\n",
      "Index 486: ID 375\n",
      "Index 487: ID 376\n",
      "Index 488: ID 377\n",
      "Index 489: ID 378\n",
      "Index 490: ID 379\n",
      "Index 491: ID 380\n",
      "Index 492: ID 381\n",
      "Index 493: ID 382\n",
      "Index 494: ID 383\n",
      "Index 495: ID 384\n",
      "Index 496: ID 385\n",
      "Index 497: ID 386\n",
      "Index 498: ID 386\n",
      "Index 499: ID 387\n",
      "Index 500: ID 388\n",
      "Index 501: ID 388\n",
      "Index 502: ID 388\n",
      "Index 503: ID 388\n",
      "Index 504: ID 388\n",
      "Index 505: ID 389\n",
      "Index 506: ID 390\n",
      "Index 507: ID 391\n",
      "Index 508: ID 391\n",
      "Index 509: ID 392\n",
      "Index 510: ID 393\n",
      "Index 511: ID None\n"
     ]
    }
   ],
   "source": [
    "test = enumerate(dataset[0]['word_ids'])\n",
    "for i in range(1):\n",
    "    for index, word_id in enumerate(dataset[0]['word_ids']):\n",
    "        print(f\"Index {index}: ID {word_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token:  [CLS]\n",
      "Label:  18\n",
      "Word_ids:  None\n",
      "Input_ids:  101\n",
      "---------------------------\n",
      "Token:  A\n",
      "Label:  18\n",
      "Word_ids:  0\n",
      "Input_ids:  138\n",
      "---------------------------\n",
      "Token:  new\n",
      "Label:  18\n",
      "Word_ids:  1\n",
      "Input_ids:  1207\n",
      "---------------------------\n",
      "Token:  ransom\n",
      "Label:  18\n",
      "Word_ids:  2\n",
      "Input_ids:  25057\n",
      "---------------------------\n",
      "Token:  ##ware\n",
      "Label:  18\n",
      "Word_ids:  2\n",
      "Input_ids:  7109\n",
      "---------------------------\n",
      "Token:  -\n",
      "Label:  18\n",
      "Word_ids:  3\n",
      "Input_ids:  118\n",
      "---------------------------\n",
      "Token:  as\n",
      "Label:  18\n",
      "Word_ids:  4\n",
      "Input_ids:  1112\n",
      "---------------------------\n",
      "Token:  -\n",
      "Label:  18\n",
      "Word_ids:  5\n",
      "Input_ids:  118\n",
      "---------------------------\n",
      "Token:  a\n",
      "Label:  18\n",
      "Word_ids:  6\n",
      "Input_ids:  170\n",
      "---------------------------\n",
      "Token:  -\n",
      "Label:  18\n",
      "Word_ids:  7\n",
      "Input_ids:  118\n",
      "---------------------------\n",
      "Token:  service\n",
      "Label:  18\n",
      "Word_ids:  8\n",
      "Input_ids:  1555\n",
      "---------------------------\n",
      "Token:  (\n",
      "Label:  18\n",
      "Word_ids:  9\n",
      "Input_ids:  113\n",
      "---------------------------\n",
      "Token:  Ra\n",
      "Label:  18\n",
      "Word_ids:  10\n",
      "Input_ids:  16890\n",
      "---------------------------\n",
      "Token:  ##a\n",
      "Label:  18\n",
      "Word_ids:  10\n",
      "Input_ids:  1161\n",
      "---------------------------\n",
      "Token:  ##S\n",
      "Label:  18\n",
      "Word_ids:  10\n",
      "Input_ids:  1708\n",
      "---------------------------\n",
      "Token:  )\n",
      "Label:  18\n",
      "Word_ids:  11\n",
      "Input_ids:  114\n",
      "---------------------------\n",
      "Token:  operation\n",
      "Label:  18\n",
      "Word_ids:  12\n",
      "Input_ids:  2805\n",
      "---------------------------\n",
      "Token:  named\n",
      "Label:  18\n",
      "Word_ids:  13\n",
      "Input_ids:  1417\n",
      "---------------------------\n",
      "Token:  C\n",
      "Label:  3\n",
      "Word_ids:  14\n",
      "Input_ids:  140\n",
      "---------------------------\n",
      "Token:  ##ica\n",
      "Label:  12\n",
      "Word_ids:  14\n",
      "Input_ids:  4578\n",
      "---------------------------\n",
      "Token:  ##da\n",
      "Label:  12\n",
      "Word_ids:  14\n",
      "Input_ids:  1810\n",
      "---------------------------\n",
      "Token:  ##33\n",
      "Label:  12\n",
      "Word_ids:  14\n",
      "Input_ids:  23493\n",
      "---------------------------\n",
      "Token:  ##01\n",
      "Label:  12\n",
      "Word_ids:  14\n",
      "Input_ids:  24400\n",
      "---------------------------\n",
      "Token:  has\n",
      "Label:  18\n",
      "Word_ids:  15\n",
      "Input_ids:  1144\n",
      "---------------------------\n",
      "Token:  already\n",
      "Label:  18\n",
      "Word_ids:  16\n",
      "Input_ids:  1640\n",
      "---------------------------\n",
      "Token:  listed\n",
      "Label:  18\n",
      "Word_ids:  17\n",
      "Input_ids:  2345\n",
      "---------------------------\n",
      "Token:  19\n",
      "Label:  18\n",
      "Word_ids:  18\n",
      "Input_ids:  1627\n",
      "---------------------------\n",
      "Token:  victims\n",
      "Label:  18\n",
      "Word_ids:  19\n",
      "Input_ids:  5256\n",
      "---------------------------\n",
      "Token:  on\n",
      "Label:  18\n",
      "Word_ids:  20\n",
      "Input_ids:  1113\n",
      "---------------------------\n",
      "Token:  its\n",
      "Label:  18\n",
      "Word_ids:  21\n",
      "Input_ids:  1157\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    print(\"Token: \", dataset[0]['tokens'][i])\n",
    "    print(\"Label: \", dataset[0]['labels'][i])\n",
    "    print(\"Word_ids: \", dataset[0]['word_ids'][i])\n",
    "    print(\"Input_ids: \", dataset[0]['input_ids'][i])\n",
    "    print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_2(data):\n",
    "    word_ids = data[\"word_ids\"]\n",
    "    original_labels = data[\"labels\"]\n",
    "    new_labels = []\n",
    "\n",
    "    previous_word_idx = None\n",
    "    \n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            new_labels.append(-100) # Special tokens\n",
    "        elif word_idx != previous_word_idx:\n",
    "            new_labels.append(original_labels[word_idx])\n",
    "        else:\n",
    "            new_labels.append(-100)\n",
    "        previous_word_idx = word_idx if word_idx is not None else previous_word_idx\n",
    "\n",
    "    data[\"labels\"] = new_labels\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f5ee0ba06c46a0a1af6510f7c5c68d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/61 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_and_aligned_dataset = dataset.map(align_labels_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', 'A', 'new', 'ransom', '##ware', '-', 'as', '-', 'a', '-', 'service', '(', 'Ra', '##a', '##S', ')', 'operation', 'named', 'C', '##ica', '##da', '##33', '##01', 'has', 'already', 'listed', '19', 'victims', 'on', 'its', 'ex', '##tor', '##tion', 'portal', ',', 'as', 'it', 'quickly', 'attacked', 'companies', 'worldwide', '.', 'The', 'new', 'c', '##y', '##ber', '##c', '##rim', '##e', 'operation', 'is', 'named', 'after', 'the', 'mysterious', '2012', '-', '2014', 'online', '/', 'real', '-', 'world', 'game', 'that', 'involved', 'elaborate', 'cry', '##pt', '##ographic', 'puzzles', 'and', 'used', 'the', 'same', 'logo', 'for', 'promotion', 'on', 'c', '##y', '##ber', '##c', '##rim', '##e', 'forums', '.', 'However', ',', 'there', \"'\", 's', 'no', 'connection', 'between', 'the', 'two', ',', 'and', 'the', 'legitimate', 'project', 'has', 'issued', 'a', 'statement', 'to', 're', '##nounce', 'any', 'association', 'and', 'con', '##de', '##m', '##n', 'the', 'ransom', '##ware', 'operators', \"'\", 'actions', '.', 'The', 'C', '##ica', '##da', '##33', '##01', 'Ra', '##a', '##S', 'first', 'began', 'promoting', 'the', 'operation', 'and', 'recruiting', 'affiliates', 'on', 'June', '29', ',', '202', '##4', ',', 'in', 'a', 'forum', 'post', 'to', 'the', 'ransom', '##ware', 'and', 'c', '##y', '##ber', '##c', '##rim', '##e', 'forum', 'known', 'as', 'RAM', '##P', '.', 'However', ',', 'B', '##lee', '##ping', '##C', '##om', '##pute', '##r', 'is', 'aware', 'of', 'C', '##ica', '##da', 'attacks', 'as', 'early', 'as', 'June', '6', ',', 'indicating', 'that', 'the', 'gang', 'was', 'operating', 'independently', 'before', 'attempting', 'to', 'recruit', 'affiliates', '.', 'Like', 'other', 'ransom', '##ware', 'operations', ',', 'C', '##ica', '##da', '##33', '##01', 'conducts', 'double', '-', 'ex', '##tor', '##tion', 'tactics', 'where', 'they', 'breach', 'corporate', 'networks', ',', 'steal', 'data', ',', 'and', 'then', 'en', '##c', '##ry', '##pt', 'devices', '.', 'The', 'encryption', 'key', 'and', 'threats', 'to', 'leak', 'stolen', 'data', 'are', 'then', 'used', 'as', 'leverage', 'to', 'scare', 'victims', 'into', 'paying', 'a', 'ransom', '.', 'The', 'threat', 'actors', 'operate', 'a', 'data', 'leak', 'site', 'that', 'is', 'used', 'as', 'part', 'of', 'their', 'double', '-', 'ex', '##tor', '##tion', 'scheme', '.', 'An', 'analysis', 'of', 'the', 'new', 'ma', '##l', '##ware', 'by', 'True', '##se', '##c', 'revealed', 'significant', 'overlap', '##s', 'between', 'C', '##ica', '##da', '##33', '##01', 'and', 'AL', '##P', '##H', '##V', '/', 'Black', '##C', '##at', ',', 'indicating', 'a', 'possible', 're', '##brand', 'or', 'a', 'fork', 'created', 'by', 'former', 'AL', '##P', '##H', '##V', \"'\", 's', 'core', 'team', 'members', '.', 'This', 'is', 'based', 'on', 'the', 'fact', 'that', ':', 'For', 'context', ',', 'AL', '##P', '##H', '##V', 'performed', 'an', 'exit', 's', '##cam', 'in', 'early', 'March', '202', '##4', 'involving', 'fake', 'claims', 'about', 'an', 'FBI', 'take', '##down', 'operation', 'after', 'they', 'stole', 'a', 'massive', '$', '22', 'million', 'payment', 'from', 'Change', 'Healthcare', 'from', 'one', 'of', 'their', 'affiliates', '.', 'True', '##se', '##c', 'has', 'also', 'found', 'indication', '##s', 'that', 'the', 'C', '##ica', '##da', '##33', '##01', 'ransom', '##ware', 'operation', 'may', 'partner', 'with', 'or', 'utilize', 'the', 'B', '##ru', '##tus', 'b', '##ot', '##net', 'for', 'initial', 'access', 'to', 'corporate', 'networks', '.', 'That', 'b', '##ot', '##net', 'was', 'previously', 'associated', 'with', 'global', '-', 'scale', 'VP', '##N', 'br', '##ute', '-', 'forcing', 'activities', 'targeting', 'C', '##isco', ',', 'Fort', '##inet', ',', 'Pa', '##lo', 'Alto', ',', 'and', 'Sonic', '##W', '##all', 'appliances', '.', 'It', \"'\", 's', 'worth', 'noting', 'that', 'the', 'B', '##ru', '##tus', 'activity', 'was', 'first', 'spotted', 'two', 'weeks', 'after', 'AL', '##P', '##H', '##V', 'shut', 'down', 'operations', ',', 'so', 'the', 'link', 'between', 'the', 'two', 'groups', 'still', 'stands', 'in', 'terms', 'of', 'timeline', '##s', '.', 'C', '##ica', '##da', '##33', '##01', 'is', 'a', 'R', '##ust', '-', 'based', '[SEP]'], 'labels': [-100, 18, 18, 18, -100, 18, 18, 18, 18, 18, 18, 18, 18, -100, -100, 18, 18, 18, 18, -100, -100, -100, -100, 18, 18, 18, 3, 12, 12, 12, 12, -100, -100, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, -100, -100, -100, -100, -100, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, -100, -100, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, -100, -100, -100, -100, -100, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, -100, 18, 18, 18, 18, -100, -100, -100, 18, 18, -100, 18, 18, 18, 18, 18, 18, -100, -100, -100, -100, 18, -100, -100, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, -100, 18, 18, 18, 18, 18, 18, 18, 18, -100, 18, 18, -100, -100, -100, -100, -100, 18, 18, 3, 12, -100, 12, 12, 12, 18, -100, -100, -100, -100, -100, -100, 18, 18, 18, 18, -100, -100, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, -100, 18, 18, 18, -100, -100, -100, -100, 18, 18, 18, 18, -100, -100, 18, 18, 18, 18, 18, 18, 6, 15, 15, 15, 15, 15, 15, -100, -100, -100, 18, 18, 18, 3, 12, 12, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 3, 12, 12, 12, 12, 1, 10, 10, 10, 10, -100, -100, 10, 10, 10, 10, 10, 10, 10, 10, -100, -100, 10, 10, -100, -100, 10, 10, 10, -100, 10, 10, -100, -100, -100, -100, 10, 10, -100, -100, -100, 10, 18, -100, -100, 18, 18, 18, 18, 18, -100, 18, 18, 18, 18, 18, 18, 18, -100, -100, -100, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, -100, -100, -100, 18, 18, 18, 18, -100, 18, 18, 18, 18, -100, 18, 18, 18, 18, 18, 18, 18, -100, 18, 18, 18, 18, 18, 18, 18, 18, 6, 15, 15, 18, 18, 18, 18, 18, 3, 12, 12, 12, -100, -100, 12, 18, 3, 12, -100, 12, 12, 18, -100, -100, -100, -100, 3, -100, 12, 12, 18, 18, 18, 18, 18, 18, -100, -100, 18, -100, -100, 18, 18, 18, 18, 18, 3, 12, 12, 12, -100, -100, 12, 12, 18, 18, 18, 18, 18, 18, -100, 18, -100, 18, 18, 18, 18, 18, -100, 18, 18, -100, 18, 3, -100, 12, 12, 12, 18, -100, -100, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, -100, -100, 18, 18, 18, 18, 6, 18, 18, 18, -100, -100, -100, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 6, 15, 18, 18, 18, 18, 18, -100, 18, 6, -100, -100, -100, -100, 15, 15, 18, -100, 18, 18, -100], 'input_ids': [101, 138, 1207, 25057, 7109, 118, 1112, 118, 170, 118, 1555, 113, 16890, 1161, 1708, 114, 2805, 1417, 140, 4578, 1810, 23493, 24400, 1144, 1640, 2345, 1627, 5256, 1113, 1157, 4252, 2772, 2116, 10823, 117, 1112, 1122, 1976, 3623, 2557, 4529, 119, 1109, 1207, 172, 1183, 3169, 1665, 10205, 1162, 2805, 1110, 1417, 1170, 1103, 8198, 1368, 118, 1387, 3294, 120, 1842, 118, 1362, 1342, 1115, 2017, 9427, 5354, 6451, 9597, 22747, 1105, 1215, 1103, 1269, 7998, 1111, 4166, 1113, 172, 1183, 3169, 1665, 10205, 1162, 25438, 119, 1438, 117, 1175, 112, 188, 1185, 3797, 1206, 1103, 1160, 117, 1105, 1103, 11582, 1933, 1144, 3010, 170, 4195, 1106, 1231, 25196, 1251, 3852, 1105, 14255, 2007, 1306, 1179, 1103, 25057, 7109, 9298, 112, 3721, 119, 1109, 140, 4578, 1810, 23493, 24400, 16890, 1161, 1708, 1148, 1310, 7495, 1103, 2805, 1105, 16226, 20795, 1113, 1340, 1853, 117, 17881, 1527, 117, 1107, 170, 13912, 2112, 1106, 1103, 25057, 7109, 1105, 172, 1183, 3169, 1665, 10205, 1162, 13912, 1227, 1112, 20898, 2101, 119, 1438, 117, 139, 6894, 2624, 1658, 4165, 22662, 1197, 1110, 4484, 1104, 140, 4578, 1810, 3690, 1112, 1346, 1112, 1340, 127, 117, 7713, 1115, 1103, 6939, 1108, 3389, 8942, 1196, 6713, 1106, 14240, 20795, 119, 2409, 1168, 25057, 7109, 2500, 117, 140, 4578, 1810, 23493, 24400, 19706, 2702, 118, 4252, 2772, 2116, 10524, 1187, 1152, 13275, 6214, 6379, 117, 8991, 2233, 117, 1105, 1173, 4035, 1665, 1616, 6451, 5197, 119, 1109, 26463, 2501, 1105, 8657, 1106, 19299, 7251, 2233, 1132, 1173, 1215, 1112, 24228, 1106, 13671, 5256, 1154, 6573, 170, 25057, 119, 1109, 4433, 5681, 4732, 170, 2233, 19299, 1751, 1115, 1110, 1215, 1112, 1226, 1104, 1147, 2702, 118, 4252, 2772, 2116, 5471, 119, 1760, 3622, 1104, 1103, 1207, 12477, 1233, 7109, 1118, 7817, 2217, 1665, 3090, 2418, 19235, 1116, 1206, 140, 4578, 1810, 23493, 24400, 1105, 18589, 2101, 3048, 2559, 120, 2117, 1658, 2980, 117, 7713, 170, 1936, 1231, 25123, 1137, 170, 13097, 1687, 1118, 1393, 18589, 2101, 3048, 2559, 112, 188, 4160, 1264, 1484, 119, 1188, 1110, 1359, 1113, 1103, 1864, 1115, 131, 1370, 5618, 117, 18589, 2101, 3048, 2559, 1982, 1126, 6300, 188, 24282, 1107, 1346, 1345, 17881, 1527, 5336, 8406, 3711, 1164, 1126, 8099, 1321, 5455, 2805, 1170, 1152, 10566, 170, 4672, 109, 1659, 1550, 7727, 1121, 9091, 22993, 1121, 1141, 1104, 1147, 20795, 119, 7817, 2217, 1665, 1144, 1145, 1276, 12754, 1116, 1115, 1103, 140, 4578, 1810, 23493, 24400, 25057, 7109, 2805, 1336, 3547, 1114, 1137, 17573, 1103, 139, 5082, 4814, 171, 3329, 6097, 1111, 3288, 2469, 1106, 6214, 6379, 119, 1337, 171, 3329, 6097, 1108, 2331, 2628, 1114, 4265, 118, 3418, 23659, 2249, 9304, 6140, 118, 6524, 2619, 15141, 140, 21097, 117, 3144, 27411, 117, 19585, 2858, 17762, 117, 1105, 16202, 2924, 5727, 26498, 119, 1135, 112, 188, 3869, 9095, 1115, 1103, 139, 5082, 4814, 3246, 1108, 1148, 6910, 1160, 2277, 1170, 18589, 2101, 3048, 2559, 3210, 1205, 2500, 117, 1177, 1103, 5088, 1206, 1103, 1160, 2114, 1253, 4061, 1107, 2538, 1104, 21169, 1116, 119, 140, 4578, 1810, 23493, 24400, 1110, 170, 155, 8954, 118, 1359, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'word_ids': [None, 0, 1, 2, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 11, 12, 13, 14, 14, 14, 14, 14, 15, 16, 17, 18, 19, 20, 21, 22, 22, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 34, 34, 34, 34, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 53, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 63, 63, 63, 63, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 86, 87, 88, 89, 90, 90, 90, 90, 91, 92, 92, 93, 94, 95, 96, 97, 98, 98, 98, 98, 98, 99, 99, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 112, 113, 114, 115, 116, 117, 118, 119, 120, 120, 121, 122, 122, 122, 122, 122, 122, 123, 124, 125, 126, 126, 127, 128, 129, 130, 130, 130, 130, 130, 130, 130, 131, 132, 133, 134, 134, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 157, 158, 159, 160, 160, 160, 160, 160, 161, 162, 163, 164, 164, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 177, 177, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 219, 219, 220, 221, 222, 223, 224, 225, 226, 227, 227, 227, 228, 229, 229, 229, 230, 231, 232, 232, 233, 234, 234, 234, 234, 234, 235, 236, 236, 236, 236, 237, 238, 238, 238, 239, 240, 241, 242, 243, 243, 244, 245, 246, 247, 248, 249, 250, 250, 250, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 268, 268, 268, 269, 270, 271, 272, 272, 273, 274, 275, 276, 276, 277, 278, 279, 280, 281, 282, 283, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 303, 303, 304, 305, 306, 307, 307, 308, 309, 310, 310, 310, 310, 310, 311, 311, 312, 313, 314, 315, 316, 317, 318, 319, 319, 319, 320, 320, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 329, 329, 330, 331, 332, 333, 334, 335, 336, 337, 337, 338, 338, 339, 340, 341, 342, 343, 343, 344, 345, 345, 346, 347, 347, 348, 349, 350, 351, 351, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 361, 361, 362, 363, 364, 365, 366, 367, 368, 369, 369, 369, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 386, 387, 388, 388, 388, 388, 388, 389, 390, 391, 391, 392, 393, None]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_and_aligned_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token:  [CLS]\n",
      "Label:  -100\n",
      "Word_ids:  None\n",
      "Input_ids:  101\n",
      "---------------------------\n",
      "Token:  A\n",
      "Label:  18\n",
      "Word_ids:  0\n",
      "Input_ids:  138\n",
      "---------------------------\n",
      "Token:  new\n",
      "Label:  18\n",
      "Word_ids:  1\n",
      "Input_ids:  1207\n",
      "---------------------------\n",
      "Token:  ransom\n",
      "Label:  18\n",
      "Word_ids:  2\n",
      "Input_ids:  25057\n",
      "---------------------------\n",
      "Token:  ##ware\n",
      "Label:  -100\n",
      "Word_ids:  2\n",
      "Input_ids:  7109\n",
      "---------------------------\n",
      "Token:  -\n",
      "Label:  18\n",
      "Word_ids:  3\n",
      "Input_ids:  118\n",
      "---------------------------\n",
      "Token:  as\n",
      "Label:  18\n",
      "Word_ids:  4\n",
      "Input_ids:  1112\n",
      "---------------------------\n",
      "Token:  -\n",
      "Label:  18\n",
      "Word_ids:  5\n",
      "Input_ids:  118\n",
      "---------------------------\n",
      "Token:  a\n",
      "Label:  18\n",
      "Word_ids:  6\n",
      "Input_ids:  170\n",
      "---------------------------\n",
      "Token:  -\n",
      "Label:  18\n",
      "Word_ids:  7\n",
      "Input_ids:  118\n",
      "---------------------------\n",
      "Token:  service\n",
      "Label:  18\n",
      "Word_ids:  8\n",
      "Input_ids:  1555\n",
      "---------------------------\n",
      "Token:  (\n",
      "Label:  18\n",
      "Word_ids:  9\n",
      "Input_ids:  113\n",
      "---------------------------\n",
      "Token:  Ra\n",
      "Label:  18\n",
      "Word_ids:  10\n",
      "Input_ids:  16890\n",
      "---------------------------\n",
      "Token:  ##a\n",
      "Label:  -100\n",
      "Word_ids:  10\n",
      "Input_ids:  1161\n",
      "---------------------------\n",
      "Token:  ##S\n",
      "Label:  -100\n",
      "Word_ids:  10\n",
      "Input_ids:  1708\n",
      "---------------------------\n",
      "Token:  )\n",
      "Label:  18\n",
      "Word_ids:  11\n",
      "Input_ids:  114\n",
      "---------------------------\n",
      "Token:  operation\n",
      "Label:  18\n",
      "Word_ids:  12\n",
      "Input_ids:  2805\n",
      "---------------------------\n",
      "Token:  named\n",
      "Label:  18\n",
      "Word_ids:  13\n",
      "Input_ids:  1417\n",
      "---------------------------\n",
      "Token:  C\n",
      "Label:  18\n",
      "Word_ids:  14\n",
      "Input_ids:  140\n",
      "---------------------------\n",
      "Token:  ##ica\n",
      "Label:  -100\n",
      "Word_ids:  14\n",
      "Input_ids:  4578\n",
      "---------------------------\n",
      "Token:  ##da\n",
      "Label:  -100\n",
      "Word_ids:  14\n",
      "Input_ids:  1810\n",
      "---------------------------\n",
      "Token:  ##33\n",
      "Label:  -100\n",
      "Word_ids:  14\n",
      "Input_ids:  23493\n",
      "---------------------------\n",
      "Token:  ##01\n",
      "Label:  -100\n",
      "Word_ids:  14\n",
      "Input_ids:  24400\n",
      "---------------------------\n",
      "Token:  has\n",
      "Label:  18\n",
      "Word_ids:  15\n",
      "Input_ids:  1144\n",
      "---------------------------\n",
      "Token:  already\n",
      "Label:  18\n",
      "Word_ids:  16\n",
      "Input_ids:  1640\n",
      "---------------------------\n",
      "Token:  listed\n",
      "Label:  18\n",
      "Word_ids:  17\n",
      "Input_ids:  2345\n",
      "---------------------------\n",
      "Token:  19\n",
      "Label:  3\n",
      "Word_ids:  18\n",
      "Input_ids:  1627\n",
      "---------------------------\n",
      "Token:  victims\n",
      "Label:  12\n",
      "Word_ids:  19\n",
      "Input_ids:  5256\n",
      "---------------------------\n",
      "Token:  on\n",
      "Label:  12\n",
      "Word_ids:  20\n",
      "Input_ids:  1113\n",
      "---------------------------\n",
      "Token:  its\n",
      "Label:  12\n",
      "Word_ids:  21\n",
      "Input_ids:  1157\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    print(\"Token: \", tokenized_and_aligned_dataset[0]['tokens'][i])\n",
    "    print(\"Label: \", tokenized_and_aligned_dataset[0]['labels'][i])\n",
    "    print(\"Word_ids: \", tokenized_and_aligned_dataset[0]['word_ids'][i])\n",
    "    print(\"Input_ids: \", tokenized_and_aligned_dataset[0]['input_ids'][i])\n",
    "    print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_word_ids(data):\n",
    "    word_ids = data[\"word_ids\"]\n",
    "    labels = data[\"labels\"]\n",
    "    new_labels = []\n",
    "\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None: #Special tokens\n",
    "            new_labels.append(-100)\n",
    "        elif word_idx != previous_word_idx: # First-subtoken\n",
    "            new_labels.append(labels[word_idx])\n",
    "        else: #Sub-token\n",
    "            new_labels.append(-100)\n",
    "\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    data[\"labels\"] = new_labels\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ff2a23f39d4c8aa67f933e8dd8b198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/61 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply the function to the dataset\n",
    "dataset = dataset.map(align_labels_with_word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', 'A', 'new', 'ransom', '##ware', '-', 'as', '-', 'a', '-', 'service', '(', 'Ra', '##a', '##S', ')', 'operation', 'named', 'C', '##ica', '##da', '##33', '##01', 'has', 'already', 'listed', '19', 'victims', 'on', 'its', 'ex', '##tor', '##tion', 'portal', ',', 'as', 'it', 'quickly', 'attacked', 'companies', 'worldwide', '.', 'The', 'new', 'c', '##y', '##ber', '##c', '##rim', '##e', 'operation', 'is', 'named', 'after', 'the', 'mysterious', '2012', '-', '2014', 'online', '/', 'real', '-', 'world', 'game', 'that', 'involved', 'elaborate', 'cry', '##pt', '##ographic', 'puzzles', 'and', 'used', 'the', 'same', 'logo', 'for', 'promotion', 'on', 'c', '##y', '##ber', '##c', '##rim', '##e', 'forums', '.', 'However', ',', 'there', \"'\", 's', 'no', 'connection', 'between', 'the', 'two', ',', 'and', 'the', 'legitimate', 'project', 'has', 'issued', 'a', 'statement', 'to', 're', '##nounce', 'any', 'association', 'and', 'con', '##de', '##m', '##n', 'the', 'ransom', '##ware', 'operators', \"'\", 'actions', '.', 'The', 'C', '##ica', '##da', '##33', '##01', 'Ra', '##a', '##S', 'first', 'began', 'promoting', 'the', 'operation', 'and', 'recruiting', 'affiliates', 'on', 'June', '29', ',', '202', '##4', ',', 'in', 'a', 'forum', 'post', 'to', 'the', 'ransom', '##ware', 'and', 'c', '##y', '##ber', '##c', '##rim', '##e', 'forum', 'known', 'as', 'RAM', '##P', '.', 'However', ',', 'B', '##lee', '##ping', '##C', '##om', '##pute', '##r', 'is', 'aware', 'of', 'C', '##ica', '##da', 'attacks', 'as', 'early', 'as', 'June', '6', ',', 'indicating', 'that', 'the', 'gang', 'was', 'operating', 'independently', 'before', 'attempting', 'to', 'recruit', 'affiliates', '.', 'Like', 'other', 'ransom', '##ware', 'operations', ',', 'C', '##ica', '##da', '##33', '##01', 'conducts', 'double', '-', 'ex', '##tor', '##tion', 'tactics', 'where', 'they', 'breach', 'corporate', 'networks', ',', 'steal', 'data', ',', 'and', 'then', 'en', '##c', '##ry', '##pt', 'devices', '.', 'The', 'encryption', 'key', 'and', 'threats', 'to', 'leak', 'stolen', 'data', 'are', 'then', 'used', 'as', 'leverage', 'to', 'scare', 'victims', 'into', 'paying', 'a', 'ransom', '.', 'The', 'threat', 'actors', 'operate', 'a', 'data', 'leak', 'site', 'that', 'is', 'used', 'as', 'part', 'of', 'their', 'double', '-', 'ex', '##tor', '##tion', 'scheme', '.', 'An', 'analysis', 'of', 'the', 'new', 'ma', '##l', '##ware', 'by', 'True', '##se', '##c', 'revealed', 'significant', 'overlap', '##s', 'between', 'C', '##ica', '##da', '##33', '##01', 'and', 'AL', '##P', '##H', '##V', '/', 'Black', '##C', '##at', ',', 'indicating', 'a', 'possible', 're', '##brand', 'or', 'a', 'fork', 'created', 'by', 'former', 'AL', '##P', '##H', '##V', \"'\", 's', 'core', 'team', 'members', '.', 'This', 'is', 'based', 'on', 'the', 'fact', 'that', ':', 'For', 'context', ',', 'AL', '##P', '##H', '##V', 'performed', 'an', 'exit', 's', '##cam', 'in', 'early', 'March', '202', '##4', 'involving', 'fake', 'claims', 'about', 'an', 'FBI', 'take', '##down', 'operation', 'after', 'they', 'stole', 'a', 'massive', '$', '22', 'million', 'payment', 'from', 'Change', 'Healthcare', 'from', 'one', 'of', 'their', 'affiliates', '.', 'True', '##se', '##c', 'has', 'also', 'found', 'indication', '##s', 'that', 'the', 'C', '##ica', '##da', '##33', '##01', 'ransom', '##ware', 'operation', 'may', 'partner', 'with', 'or', 'utilize', 'the', 'B', '##ru', '##tus', 'b', '##ot', '##net', 'for', 'initial', 'access', 'to', 'corporate', 'networks', '.', 'That', 'b', '##ot', '##net', 'was', 'previously', 'associated', 'with', 'global', '-', 'scale', 'VP', '##N', 'br', '##ute', '-', 'forcing', 'activities', 'targeting', 'C', '##isco', ',', 'Fort', '##inet', ',', 'Pa', '##lo', 'Alto', ',', 'and', 'Sonic', '##W', '##all', 'appliances', '.', 'It', \"'\", 's', 'worth', 'noting', 'that', 'the', 'B', '##ru', '##tus', 'activity', 'was', 'first', 'spotted', 'two', 'weeks', 'after', 'AL', '##P', '##H', '##V', 'shut', 'down', 'operations', ',', 'so', 'the', 'link', 'between', 'the', 'two', 'groups', 'still', 'stands', 'in', 'terms', 'of', 'timeline', '##s', '.', 'C', '##ica', '##da', '##33', '##01', 'is', 'a', 'R', '##ust', '-', 'based', '[SEP]'], 'labels': [-100, 18, 18, 18, -100, 18, 18, 18, 18, 18, 18, 18, 18, -100, -100, 18, 18, 18, 18, -100, -100, -100, -100, 18, 18, 18, 3, 12, 12, 12, 12, -100, -100, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, -100, -100, -100, -100, -100, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, -100, -100, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, -100, -100, -100, -100, -100, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, -100, 18, 18, 18, 18, -100, -100, -100, 18, 18, -100, 18, 18, 18, 18, 18, 18, -100, -100, -100, -100, 18, -100, -100, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, -100, 18, 18, 18, 18, 18, 18, 18, 18, -100, 18, 18, -100, -100, -100, -100, -100, 18, 18, 3, 12, -100, 12, 12, 12, 18, -100, -100, -100, -100, -100, -100, 18, 18, 18, 18, -100, -100, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, -100, 18, 18, 18, -100, -100, -100, -100, 18, 18, 18, 18, -100, -100, 18, 18, 18, 18, 18, 18, 6, 15, 15, 15, 15, 15, 15, -100, -100, -100, 18, 18, 18, 3, 12, 12, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 3, 12, 12, 12, 12, 1, 10, 10, 10, 10, -100, -100, 10, 10, 10, 10, 10, 10, 10, 10, -100, -100, 10, 10, -100, -100, 10, 10, 10, -100, 10, 10, -100, -100, -100, -100, 10, 10, -100, -100, -100, 10, 18, -100, -100, 18, 18, 18, 18, 18, -100, 18, 18, 18, 18, 18, 18, 18, -100, -100, -100, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, -100, -100, -100, 18, 18, 18, 18, -100, 18, 18, 18, 18, -100, 18, 18, 18, 18, 18, 18, 18, -100, 18, 18, 18, 18, 18, 18, 18, 18, 6, 15, 15, 18, 18, 18, 18, 18, 3, 12, 12, 12, -100, -100, 12, 18, 3, 12, -100, 12, 12, 18, -100, -100, -100, -100, 3, -100, 12, 12, 18, 18, 18, 18, 18, 18, -100, -100, 18, -100, -100, 18, 18, 18, 18, 18, 3, 12, 12, 12, -100, -100, 12, 12, 18, 18, 18, 18, 18, 18, -100, 18, -100, 18, 18, 18, 18, 18, -100, 18, 18, -100, 18, 3, -100, 12, 12, 12, 18, -100, -100, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, -100, -100, 18, 18, 18, 18, 6, 18, 18, 18, -100, -100, -100, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 6, 15, 18, 18, 18, 18, 18, -100, 18, 6, -100, -100, -100, -100, 15, 15, 18, -100, 18, 18, -100], 'input_ids': [101, 138, 1207, 25057, 7109, 118, 1112, 118, 170, 118, 1555, 113, 16890, 1161, 1708, 114, 2805, 1417, 140, 4578, 1810, 23493, 24400, 1144, 1640, 2345, 1627, 5256, 1113, 1157, 4252, 2772, 2116, 10823, 117, 1112, 1122, 1976, 3623, 2557, 4529, 119, 1109, 1207, 172, 1183, 3169, 1665, 10205, 1162, 2805, 1110, 1417, 1170, 1103, 8198, 1368, 118, 1387, 3294, 120, 1842, 118, 1362, 1342, 1115, 2017, 9427, 5354, 6451, 9597, 22747, 1105, 1215, 1103, 1269, 7998, 1111, 4166, 1113, 172, 1183, 3169, 1665, 10205, 1162, 25438, 119, 1438, 117, 1175, 112, 188, 1185, 3797, 1206, 1103, 1160, 117, 1105, 1103, 11582, 1933, 1144, 3010, 170, 4195, 1106, 1231, 25196, 1251, 3852, 1105, 14255, 2007, 1306, 1179, 1103, 25057, 7109, 9298, 112, 3721, 119, 1109, 140, 4578, 1810, 23493, 24400, 16890, 1161, 1708, 1148, 1310, 7495, 1103, 2805, 1105, 16226, 20795, 1113, 1340, 1853, 117, 17881, 1527, 117, 1107, 170, 13912, 2112, 1106, 1103, 25057, 7109, 1105, 172, 1183, 3169, 1665, 10205, 1162, 13912, 1227, 1112, 20898, 2101, 119, 1438, 117, 139, 6894, 2624, 1658, 4165, 22662, 1197, 1110, 4484, 1104, 140, 4578, 1810, 3690, 1112, 1346, 1112, 1340, 127, 117, 7713, 1115, 1103, 6939, 1108, 3389, 8942, 1196, 6713, 1106, 14240, 20795, 119, 2409, 1168, 25057, 7109, 2500, 117, 140, 4578, 1810, 23493, 24400, 19706, 2702, 118, 4252, 2772, 2116, 10524, 1187, 1152, 13275, 6214, 6379, 117, 8991, 2233, 117, 1105, 1173, 4035, 1665, 1616, 6451, 5197, 119, 1109, 26463, 2501, 1105, 8657, 1106, 19299, 7251, 2233, 1132, 1173, 1215, 1112, 24228, 1106, 13671, 5256, 1154, 6573, 170, 25057, 119, 1109, 4433, 5681, 4732, 170, 2233, 19299, 1751, 1115, 1110, 1215, 1112, 1226, 1104, 1147, 2702, 118, 4252, 2772, 2116, 5471, 119, 1760, 3622, 1104, 1103, 1207, 12477, 1233, 7109, 1118, 7817, 2217, 1665, 3090, 2418, 19235, 1116, 1206, 140, 4578, 1810, 23493, 24400, 1105, 18589, 2101, 3048, 2559, 120, 2117, 1658, 2980, 117, 7713, 170, 1936, 1231, 25123, 1137, 170, 13097, 1687, 1118, 1393, 18589, 2101, 3048, 2559, 112, 188, 4160, 1264, 1484, 119, 1188, 1110, 1359, 1113, 1103, 1864, 1115, 131, 1370, 5618, 117, 18589, 2101, 3048, 2559, 1982, 1126, 6300, 188, 24282, 1107, 1346, 1345, 17881, 1527, 5336, 8406, 3711, 1164, 1126, 8099, 1321, 5455, 2805, 1170, 1152, 10566, 170, 4672, 109, 1659, 1550, 7727, 1121, 9091, 22993, 1121, 1141, 1104, 1147, 20795, 119, 7817, 2217, 1665, 1144, 1145, 1276, 12754, 1116, 1115, 1103, 140, 4578, 1810, 23493, 24400, 25057, 7109, 2805, 1336, 3547, 1114, 1137, 17573, 1103, 139, 5082, 4814, 171, 3329, 6097, 1111, 3288, 2469, 1106, 6214, 6379, 119, 1337, 171, 3329, 6097, 1108, 2331, 2628, 1114, 4265, 118, 3418, 23659, 2249, 9304, 6140, 118, 6524, 2619, 15141, 140, 21097, 117, 3144, 27411, 117, 19585, 2858, 17762, 117, 1105, 16202, 2924, 5727, 26498, 119, 1135, 112, 188, 3869, 9095, 1115, 1103, 139, 5082, 4814, 3246, 1108, 1148, 6910, 1160, 2277, 1170, 18589, 2101, 3048, 2559, 3210, 1205, 2500, 117, 1177, 1103, 5088, 1206, 1103, 1160, 2114, 1253, 4061, 1107, 2538, 1104, 21169, 1116, 119, 140, 4578, 1810, 23493, 24400, 1110, 170, 155, 8954, 118, 1359, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'word_ids': [None, 0, 1, 2, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 11, 12, 13, 14, 14, 14, 14, 14, 15, 16, 17, 18, 19, 20, 21, 22, 22, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 34, 34, 34, 34, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 53, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 63, 63, 63, 63, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 86, 87, 88, 89, 90, 90, 90, 90, 91, 92, 92, 93, 94, 95, 96, 97, 98, 98, 98, 98, 98, 99, 99, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 112, 113, 114, 115, 116, 117, 118, 119, 120, 120, 121, 122, 122, 122, 122, 122, 122, 123, 124, 125, 126, 126, 127, 128, 129, 130, 130, 130, 130, 130, 130, 130, 131, 132, 133, 134, 134, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 157, 158, 159, 160, 160, 160, 160, 160, 161, 162, 163, 164, 164, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 177, 177, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 219, 219, 220, 221, 222, 223, 224, 225, 226, 227, 227, 227, 228, 229, 229, 229, 230, 231, 232, 232, 233, 234, 234, 234, 234, 234, 235, 236, 236, 236, 236, 237, 238, 238, 238, 239, 240, 241, 242, 243, 243, 244, 245, 246, 247, 248, 249, 250, 250, 250, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 268, 268, 268, 269, 270, 271, 272, 272, 273, 274, 275, 276, 276, 277, 278, 279, 280, 281, 282, 283, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 303, 303, 304, 305, 306, 307, 307, 308, 309, 310, 310, 310, 310, 310, 311, 311, 312, 313, 314, 315, 316, 317, 318, 319, 319, 319, 320, 320, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 329, 329, 330, 331, 332, 333, 334, 335, 336, 337, 337, 338, 338, 339, 340, 341, 342, 343, 343, 344, 345, 345, 346, 347, 347, 348, 349, 350, 351, 351, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 361, 361, 362, 363, 364, 365, 366, 367, 368, 369, 369, 369, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 386, 387, 388, 388, 388, 388, 388, 389, 390, 391, 391, 392, 393, None]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b60ed266604da9a7d6ee7f22aa9631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/61 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Sample Debugging ====\n",
      "Len examples:  512\n",
      "Len tokenized inputs:  692\n",
      "Original Tokens:  ['[CLS]', 'Nuclear', 'waste', 'processing', 'facility', 'Se', '##lla', '##field', 'has', 'been', 'fined', '£', '##33', '##2', ',', '500', '(', '$', '440', '##k', ')', 'by', 'the', 'Office', 'for', 'Nuclear', 'Regulation', '(', 'ON', '##R', ')', 'for', 'failing', 'to', 'ad', '##here', 'to', 'c', '##y', '##bers', '##ec', '##urity', 'standards', 'and', 'putting', 'sensitive', 'nuclear', 'information', 'at', 'risk', 'over', 'four', 'years', ',', 'from', '2019', 'to', '202', '##3', '.', 'According', 'to', 'the', 'ON', '##R', 'announcement', ',', 'Se', '##lla', '##field', 'failed', 'to', 'follow', 'its', 'own', 'approved', 'c', '##y', '##bers', '##ec', '##urity', 'protocols', 'by', 'leaving', 'multiple', 'v', '##ul', '##ner', '##abi', '##lities', 'in', 'its', 'IT', 'systems', 'un', '##pa', '##tched', ',', 'violating', 'the', 'Nuclear', 'Industries', 'Security', 'Regulations', '2003', '.', 'Although', 'no', 'exploitation', 'has', 'occurred', ',', 'the', 'weaknesses', 'exposed', 'the', 'facility', 'to', 'risks', 'such', 'as', 'ransom', '##ware', ',', 'p', '##his', '##hing', ',', 'and', 'potential', 'data', 'loss', ',', 'which', 'could', 'disrupt', 'high', '-', 'hazard', 'operations', 'and', 'delay', 'de', '##com', '##mission', '##ing', 'work', '.', 'Se', '##lla', '##field', 'is', 'one', 'of', 'Europe', \"'\", 's', 'largest', 'nuclear', 'facilities', ',', 'located', 'in', 'C', '##umbria', ',', 'UK', '.', 'It', 'plays', 'a', 'significant', 'role', 'in', 'managing', 'and', 'processing', 'radioactive', 'materials', ',', 'handling', 'more', 'nuclear', 'waste', 'in', 'one', 'location', 'than', 'any', 'other', 'facility', 'worldwide', '.', 'The', 'site', 'is', 'involved', 'in', 're', '##tri', '##ev', '##ing', 'nuclear', 'waste', ',', 'fuel', ',', 'and', 's', '##lu', '##dge', 'from', 'legacy', 'ponds', 'and', 'si', '##los', ',', 'storing', 'radioactive', 'materials', 'such', 'as', 'p', '##lut', '##onium', 'and', 'uranium', ',', 'managing', 'spent', 'nuclear', 'fuel', 'rods', ',', 'and', 're', '##media', '##ting', 'and', 'de', '##com', '##mission', '##ing', 'nuclear', 'facilities', '.', 'Se', '##lla', '##field', 'is', 'a', 'critical', 'unit', 'for', 'the', 'UK', \"'\", 's', 'nuclear', 'waste', 'management', 'system', ',', 'so', 'its', 'IT', 'systems', 'security', 'is', 'vital', 'to', 'ensure', 'safe', 'operations', '.', 'Last', 'year', ',', 'a', 'series', 'of', 'investigations', 'by', 'The', 'Guardian', 'into', 'Se', '##lla', '##field', \"'\", 's', 'c', '##y', '##bers', '##ec', '##urity', 'brought', 'attention', 'to', 'multiple', 'severe', 'issues', ',', 'revealing', 'that', 'contractors', 'had', 'easy', 'access', 'to', 'critical', 'systems', 'where', 'they', ',', 'among', 'other', 'things', ',', 'could', 'install', 'USB', 'drives', '.', 'Additionally', ',', 'well', '-', 'known', 'v', '##ul', '##ner', '##abi', '##lities', 'within', 'the', 'facility', 'a', '##bound', ',', 'giving', 'the', 'site', 'the', 'nickname', '\"', 'Vol', '##de', '##mor', '##t', '\"', 'by', 'people', 'working', 'there', '.', 'An', 'audit', 'from', 'French', 'security', 'firm', 'At', '##os', 'revealed', 'that', 'roughly', '75', '%', 'of', 'Se', '##lla', '##field', \"'\", 's', 'servers', 'were', 'vulnerable', 'to', 'attacks', 'with', 'potentially', 'catastrophic', 'consequences', '.', 'The', 'nuclear', 'site', \"'\", 's', 'operators', 'pleaded', 'guilty', 'in', 'June', '202', '##4', 'to', 'their', 'failure', 'to', 'comply', 'with', 'standard', 'IT', 'security', 'regulations', ',', 'admitting', 'their', 'failure', '.', 'ON', '##R', 'investigated', 'these', 'reports', ',', 'and', 'while', 'it', 'confirmed', 'that', 'Se', '##lla', '##field', 'failed', 'to', 'a', '##bide', 'by', 'the', 'c', '##y', '##bers', '##ec', '##urity', 'standards', 'that', 'under', '##pin', 'the', 'operation', 'of', 'such', 'sites', 'in', 'the', 'UK', ',', 'it', 'says', 'it', 'found', 'no', 'evidence', 'that', 'the', 'v', '##ul', '##ner', '##abi', '##lities', 'were', 'leverage', '##d', 'in', 'attacks', '.', 'This', 'contrasts', 'previous', 'reports', 'by', 'the', 'press', 'that', 'Russian', 'and', 'Chinese', 'ha', '##ckers', 'allegedly', 'planted', 'ma', '##l', '##ware', 'on', 'the', 'site', ',', 'and', 'that', 'security', 'breach', '##es', 'occurred', 'as', 'far', 'back', 'as', '2015', '.', '\"', 'An', 'investigation', 'by', 'ON', '##R', '[', '[SEP]']\n",
      "decoded:  ['[CLS]', '[CLS]', 'N', 'uclear', 'was', 'te', 'processing', 'fac', 'ility', 'Se', '##', 'lla', '##', 'field', 'has', 'been', 'f', 'ined', '£', '##', '33', '##', '2', ',', '500', '(', '$', '440', '##', 'k', ')', 'by', 'the', 'Office', 'for', 'N', 'uclear', 'Reg', 'ulation', '(', 'ON', '##', 'R', ')', 'for', 'f', 'ailing', 'to', 'ad', '##', 'here', 'to', 'c', '##', 'y', '##', 'bers', '##', 'ec', '##', 'urity', 'stand', 'ards', 'and', 'put', 'ting', 'sensitive', 'nuclear', 'information', 'at', 'risk', 'over', 'four', 'years', ',', 'from', '2019', 'to', '202', '##', '3', '.', 'According', 'to', 'the', 'ON', '##', 'R', 'ann', 'ounce', 'ment', ',', 'Se', '##', 'lla', '##', 'field', 'failed', 'to', 'follow', 'its', 'own', 'approved', 'c', '##', 'y', '##', 'bers', '##', 'ec', '##', 'urity', 'prot', 'ocols', 'by', 'le', 'aving', 'multiple', 'v', '##', 'ul', '##', 'ner', '##', 'abi', '##', 'l', 'ities', 'in', 'its', 'IT', 'systems', 'un', '##', 'pa', '##', 't', 'ched', ',', 'viol', 'ating', 'the', 'N', 'uclear', 'Indust', 'ries', 'Security', 'Reg', 'ulations', '2003', '.', 'Although', 'no', 'expl', 'o', 'itation', 'has', 'occur', 'red', ',', 'the', 'weak', 'ness', 'es', 'exposed', 'the', 'fac', 'ility', 'to', 'ris', 'ks', 'such', 'as', 'rans', 'om', '##', 'ware', ',', 'p', '##', 'his', '##', 'hing', ',', 'and', 'potential', 'data', 'loss', ',', 'which', 'could', 'dis', 'rupt', 'high', '-', 'h', 'azard', 'operations', 'and', 'delay', 'de', '##', 'com', '##', 'mission', '##', 'ing', 'work', '.', 'Se', '##', 'lla', '##', 'field', 'is', 'one', 'of', 'Europe', \"'\", 's', 'largest', 'nuclear', 'fac', 'ilities', ',', 'located', 'in', 'C', '##', 'umb', 'ria', ',', 'UK', '.', 'It', 'plays', 'a', 'significant', 'role', 'in', 'man', 'aging', 'and', 'processing', 'radio', 'active', 'materials', ',', 'handling', 'more', 'nuclear', 'was', 'te', 'in', 'one', 'location', 'than', 'any', 'other', 'fac', 'ility', 'world', 'wide', '.', 'The', 'site', 'is', 'inv', 'olved', 'in', 're', '##', 'tri', '##', 'ev', '##', 'ing', 'nuclear', 'was', 'te', ',', 'fuel', ',', 'and', 's', '##', 'lu', '##', 'd', 'ge', 'from', 'leg', 'acy', 'p', 'onds', 'and', 'si', '##', 'los', ',', 'st', 'oring', 'radio', 'active', 'materials', 'such', 'as', 'p', '##', 'lut', '##', 'onium', 'and', 'ur', 'anium', ',', 'man', 'aging', 'sp', 'ent', 'nuclear', 'fuel', 'ro', 'ds', ',', 'and', 're', '##', 'media', '##', 'ting', 'and', 'de', '##', 'com', '##', 'mission', '##', 'ing', 'nuclear', 'fac', 'ilities', '.', 'Se', '##', 'lla', '##', 'field', 'is', 'a', 'critical', 'unit', 'for', 'the', 'UK', \"'\", 's', 'nuclear', 'was', 'te', 'management', 'system', ',', 'so', 'its', 'IT', 'systems', 'security', 'is', 'v', 'ital', 'to', 'ensure', 'safe', 'operations', '.', 'Last', 'year', ',', 'a', 'series', 'of', 'invest', 'ig', 'ations', 'by', 'The', 'Guard', 'ian', 'into', 'Se', '##', 'lla', '##', 'field', \"'\", 's', 'c', '##', 'y', '##', 'bers', '##', 'ec', '##', 'urity', 'br', 'ought', 'attention', 'to', 'multiple', 'severe', 'issues', ',', 'reve', 'aling', 'that', 'contract', 'ors', 'had', 'easy', 'access', 'to', 'critical', 'systems', 'where', 'they', ',', 'among', 'other', 'things', ',', 'could', 'install', 'USB', 'driv', 'es', '.', 'Additionally', ',', 'well', '-', 'known', 'v', '##', 'ul', '##', 'ner', '##', 'abi', '##', 'l', 'ities', 'within', 'the', 'fac', 'ility', 'a', '##', 'bound', ',', 'giving', 'the', 'site', 'the', 'nick', 'name', '\"', 'Vol', '##', 'de', '##', 'mor', '##', 't', '\"', 'by', 'people', 'working', 'there', '.', 'An', 'aud', 'it', 'from', 'French', 'security', 'firm', 'At', '##', 'os', 'reve', 'aled', 'that', 'rough', 'ly', '75', '%', 'of', 'Se', '##', 'lla', '##', 'field', \"'\", 's', 'ser', 'vers', 'were', 'v', 'ul', 'ner', 'able', 'to', 'att', 'acks', 'with', 'pot', 'entially', 'cat', 'ast', 'rophic', 'con', 'sequences', '.', 'The', 'nuclear', 'site', \"'\", 's', 'oper', 'ators', 'ple', 'aded', 'gu', 'ilty', 'in', 'June', '202', '##', '4', 'to', 'their', 'failure', 'to', 'com', 'ply', 'with', 'standard', 'IT', 'security', 'reg', 'ulations', ',', 'ad', 'mitting', 'their', 'failure', '.', 'ON', '##', 'R', 'invest', 'igated', 'these', 'reports', ',', 'and', 'while', 'it', 'confirmed', 'that', 'Se', '##', 'lla', '##', 'field', 'failed', 'to', 'a', '##', 'b', 'ide', 'by', 'the', 'c', '##', 'y', '##', 'bers', '##', 'ec', '##', 'urity', 'stand', 'ards', 'that', 'under', '##', 'pin', 'the', 'operation', 'of', 'such', 'sites', 'in', 'the', 'UK', ',', 'it', 's', 'ays', 'it', 'found', 'no', 'evidence', 'that', 'the', 'v', '##', 'ul', '##', 'ner', '##', 'abi', '##', 'l', 'ities', 'were', 'le', 'verage', '##', 'd', 'in', 'att', 'acks', '.', 'This', 'contr', 'asts', 'previous', 'reports', 'by', 'the', 'press', 'that', 'Russian', 'and', 'Chinese', 'ha', '##', 'ck', 'ers', 'alleg', 'edly', 'plant', 'ed', 'ma', '##', 'l', '##', 'ware', 'on', 'the', 'site', ',', 'and', 'that', 'security', 'bre', 'ach', '##', 'es', 'occur', 'red', 'as', 'far', 'back', 'as', '2015', '.', '\"', 'An', 'invest', 'igation', 'by', 'ON', '##', 'R', '[', '[SEP]', '[SEP]']\n",
      "Tokenized Input IDs:  [50281, 50281, 47, 13706, 4238, 442, 21678, 28402, 874, 3251, 817, 14797, 817, 3423, 7110, 20394, 71, 967, 14775, 817, 1610, 817, 19, 13, 5388, 9, 5, 31543, 817, 76, 10, 1615, 783, 33577, 1542, 47, 13706, 5785, 1427, 9, 1139, 817, 51, 10, 1542, 71, 13454, 936, 324, 817, 1568, 936, 68, 817, 90, 817, 1653, 817, 886, 817, 5051, 1676, 2196, 395, 1065, 1076, 19579, 47745, 18480, 255, 16272, 1189, 12496, 10526, 13, 4064, 9638, 936, 18161, 817, 20, 15, 7130, 936, 783, 1139, 817, 51, 1136, 12943, 420, 13, 3251, 817, 14797, 817, 3423, 27337, 936, 25739, 953, 628, 37407, 68, 817, 90, 817, 1653, 817, 886, 817, 5051, 10075, 12862, 1615, 282, 3292, 34263, 87, 817, 335, 817, 1216, 817, 18754, 817, 77, 1005, 249, 953, 1433, 39098, 328, 817, 4904, 817, 85, 2147, 13, 23283, 839, 783, 47, 13706, 47219, 2246, 20356, 5785, 3339, 9755, 15, 8430, 2369, 15083, 80, 3535, 7110, 30714, 433, 13, 783, 20881, 1255, 265, 36330, 783, 28402, 874, 936, 4448, 661, 10328, 284, 16147, 297, 817, 1935, 13, 81, 817, 8701, 817, 2027, 13, 395, 33177, 2203, 18585, 13, 4609, 16534, 3431, 3787, 8656, 14, 73, 38364, 42316, 395, 29000, 615, 817, 681, 817, 2230, 817, 272, 1601, 15, 3251, 817, 14797, 817, 3423, 261, 531, 1171, 18913, 8, 84, 45242, 47745, 28402, 3191, 13, 46033, 249, 36, 817, 3561, 5182, 13, 19642, 15, 1147, 42241, 66, 32258, 14337, 249, 1342, 2977, 395, 21678, 25337, 4507, 11310, 13, 48590, 3062, 47745, 4238, 442, 249, 531, 12428, 14644, 1279, 977, 28402, 874, 10186, 4363, 15, 510, 11264, 261, 7821, 5336, 249, 250, 817, 12512, 817, 1173, 817, 272, 47745, 4238, 442, 13, 36667, 13, 395, 84, 817, 7675, 817, 69, 463, 4064, 1851, 1974, 81, 39320, 395, 9245, 817, 26185, 13, 296, 4263, 25337, 4507, 11310, 10328, 284, 81, 817, 40376, 817, 37583, 395, 321, 19808, 13, 1342, 2977, 1033, 290, 47745, 36667, 287, 1397, 13, 395, 250, 817, 8236, 817, 1076, 395, 615, 817, 681, 817, 2230, 817, 272, 47745, 28402, 3191, 15, 3251, 817, 14797, 817, 3423, 261, 66, 26717, 8522, 1542, 783, 19642, 8, 84, 47745, 4238, 442, 26454, 10394, 13, 601, 953, 1433, 39098, 13980, 261, 87, 1562, 936, 5358, 22768, 42316, 15, 8693, 2913, 13, 66, 22253, 1171, 24889, 304, 569, 1615, 510, 35854, 757, 14806, 3251, 817, 14797, 817, 3423, 8, 84, 68, 817, 90, 817, 1653, 817, 886, 817, 5051, 1288, 1224, 42959, 936, 34263, 40192, 22402, 13, 38198, 4052, 3529, 20987, 641, 10178, 36423, 10773, 936, 26717, 39098, 2811, 9328, 13, 35094, 977, 28579, 13, 16534, 12543, 26346, 14615, 265, 15, 28144, 13, 4714, 14, 4304, 87, 817, 335, 817, 1216, 817, 18754, 817, 77, 1005, 24767, 783, 28402, 874, 66, 817, 9458, 13, 19647, 783, 11264, 783, 23237, 1590, 3, 15271, 817, 615, 817, 20014, 817, 85, 3, 1615, 13174, 21107, 9088, 15, 1145, 5353, 262, 4064, 21258, 13980, 49053, 3404, 817, 375, 38198, 3256, 3529, 903, 314, 1976, 6, 1171, 3251, 817, 14797, 817, 3423, 8, 84, 2152, 735, 12796, 87, 335, 1216, 494, 936, 1595, 7305, 3113, 11714, 4303, 8076, 505, 16117, 585, 45997, 15, 510, 47745, 11264, 8, 84, 2211, 2392, 713, 7917, 4297, 7443, 249, 18948, 18161, 817, 21, 936, 14094, 33699, 936, 681, 2881, 3113, 15291, 1433, 13980, 1747, 3339, 13, 324, 15318, 14094, 33699, 15, 1139, 817, 51, 24889, 27285, 20513, 41071, 13, 395, 6050, 262, 37467, 3529, 3251, 817, 14797, 817, 3423, 27337, 936, 66, 817, 67, 504, 1615, 783, 68, 817, 90, 817, 1653, 817, 886, 817, 5051, 1676, 2196, 3529, 4524, 817, 9852, 783, 20936, 1171, 10328, 37813, 249, 783, 19642, 13, 262, 84, 698, 262, 14541, 2369, 22432, 3529, 783, 87, 817, 335, 817, 1216, 817, 18754, 817, 77, 1005, 12796, 282, 2394, 817, 69, 249, 1595, 7305, 15, 1552, 19657, 9346, 35065, 41071, 1615, 783, 7100, 3529, 27397, 395, 27223, 3227, 817, 777, 398, 38653, 15376, 8186, 264, 785, 817, 77, 817, 1935, 251, 783, 11264, 13, 395, 3529, 13980, 3381, 607, 817, 265, 30714, 433, 284, 14103, 2135, 284, 6620, 15, 3, 1145, 24889, 5538, 1615, 1139, 817, 51, 60, 50282, 50282]\n",
      "Word IDs Mapping:  [None, 0, 1, 1, 2, 2, 3, 4, 4, 5, 6, 6, 7, 7, 8, 9, 10, 10, 11, 12, 12, 13, 13, 14, 15, 16, 17, 18, 19, 19, 20, 21, 22, 23, 24, 25, 25, 26, 26, 27, 28, 29, 29, 30, 31, 32, 32, 33, 34, 35, 35, 36, 37, 38, 38, 39, 39, 40, 40, 41, 41, 42, 42, 43, 44, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 58, 59, 60, 61, 62, 63, 64, 64, 65, 65, 65, 66, 67, 68, 68, 69, 69, 70, 71, 72, 73, 74, 75, 76, 77, 77, 78, 78, 79, 79, 80, 80, 81, 81, 82, 83, 83, 84, 85, 86, 86, 87, 87, 88, 88, 89, 89, 89, 90, 91, 92, 93, 94, 95, 95, 96, 96, 96, 97, 98, 98, 99, 100, 100, 101, 101, 102, 103, 103, 104, 105, 106, 107, 108, 108, 108, 109, 110, 110, 111, 112, 113, 113, 113, 114, 115, 116, 116, 117, 118, 118, 119, 120, 121, 121, 122, 122, 123, 124, 125, 125, 126, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 135, 136, 137, 138, 138, 139, 140, 141, 142, 143, 143, 144, 144, 145, 145, 146, 147, 148, 149, 149, 150, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 159, 160, 161, 162, 163, 164, 164, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 174, 175, 176, 177, 177, 178, 179, 180, 181, 182, 183, 183, 184, 185, 186, 187, 188, 189, 190, 190, 191, 191, 192, 193, 194, 195, 196, 196, 197, 198, 199, 199, 200, 200, 201, 201, 202, 203, 203, 204, 205, 206, 207, 208, 209, 209, 210, 210, 210, 211, 212, 212, 213, 213, 214, 215, 216, 216, 217, 218, 218, 219, 219, 220, 221, 222, 223, 224, 224, 225, 225, 226, 227, 227, 228, 229, 229, 230, 230, 231, 232, 233, 233, 234, 235, 236, 237, 237, 238, 238, 239, 240, 241, 241, 242, 242, 243, 243, 244, 245, 245, 246, 247, 248, 248, 249, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 282, 282, 283, 284, 285, 285, 286, 287, 288, 288, 289, 289, 290, 291, 292, 293, 293, 294, 294, 295, 295, 296, 296, 297, 297, 298, 299, 300, 301, 302, 303, 304, 304, 305, 306, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 323, 324, 325, 326, 327, 328, 329, 330, 331, 331, 332, 332, 333, 333, 334, 334, 334, 335, 336, 337, 337, 338, 339, 339, 340, 341, 342, 343, 344, 345, 345, 346, 347, 348, 348, 349, 349, 350, 350, 351, 352, 353, 354, 355, 356, 357, 358, 358, 359, 360, 361, 362, 363, 364, 364, 365, 365, 366, 367, 367, 368, 369, 370, 371, 372, 372, 373, 373, 374, 375, 376, 376, 377, 378, 378, 378, 378, 379, 380, 380, 381, 382, 382, 383, 383, 383, 384, 384, 385, 386, 387, 388, 389, 390, 391, 391, 392, 392, 393, 393, 394, 395, 396, 397, 397, 398, 399, 400, 401, 402, 402, 403, 404, 405, 406, 407, 407, 408, 409, 409, 410, 411, 412, 413, 414, 414, 415, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 425, 426, 426, 427, 428, 429, 430, 430, 430, 431, 432, 433, 434, 434, 435, 435, 436, 436, 437, 437, 438, 438, 439, 440, 441, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 452, 453, 454, 455, 456, 457, 458, 459, 460, 460, 461, 461, 462, 462, 463, 463, 463, 464, 465, 465, 466, 466, 467, 468, 468, 469, 470, 471, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 482, 482, 483, 483, 484, 484, 485, 486, 486, 487, 487, 488, 489, 490, 491, 492, 493, 494, 495, 495, 496, 496, 497, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 506, 507, 508, 509, 509, 510, 511, None]\n",
      "Aligned Labels:  [-100, 14, 11, -100, 11, -100, 11, 11, -100, 11, 11, -100, 11, -100, 11, 11, 11, -100, 11, 11, -100, 11, -100, 11, 11, 11, 11, 11, 11, -100, 11, 11, 11, 11, 11, 11, -100, 11, -100, 11, 11, 11, -100, 11, 11, 11, -100, 11, 11, 11, -100, 11, 11, 11, -100, 11, -100, 11, -100, 11, -100, 11, -100, 11, 11, -100, 11, 11, 11, 11, 11, 16, 16, 16, 16, 16, 16, 16, 16, 16, -100, 16, 16, 16, 16, 10, 15, -100, 16, -100, -100, 16, 10, 15, -100, 15, -100, 16, 16, 16, 16, 16, 16, 16, 16, -100, 16, -100, 16, -100, 16, -100, 16, -100, 16, 16, -100, 16, 16, 16, -100, 16, -100, 16, -100, 16, -100, -100, 16, 16, 16, 16, 16, 16, -100, 16, -100, -100, 16, 16, -100, 16, 10, -100, 15, -100, 15, 15, -100, 16, 16, 16, 16, 16, -100, -100, 16, 16, -100, 16, 16, 16, -100, -100, 16, 16, 16, -100, 16, 16, -100, 16, 16, 16, -100, 16, -100, 16, 16, 16, -100, 16, -100, 16, 16, 16, 16, 16, 16, 16, 16, 16, -100, 16, 16, 16, -100, 16, 16, 16, 16, 16, -100, 16, -100, 16, -100, 16, 16, 10, 15, -100, 15, -100, 16, 16, 16, 6, 5, 5, 16, 16, 16, -100, 16, 16, 16, 6, 5, -100, -100, 5, 5, 16, 16, 16, 16, 16, 16, 16, 16, -100, 16, 16, 16, -100, 16, 16, 16, 16, 16, 16, -100, 16, 16, 16, 16, 16, 16, 16, -100, 16, -100, 16, 16, 16, 16, 16, -100, 16, 16, 16, -100, 16, -100, 16, -100, 16, 16, -100, 16, 16, 16, 16, 16, 16, -100, 16, -100, -100, 16, 16, -100, 16, -100, 16, 16, 16, -100, 16, 16, -100, 16, -100, 16, 16, 16, 16, 16, -100, 16, -100, 16, 16, -100, 16, 16, -100, 16, -100, 16, 16, 16, -100, 16, 16, 16, 16, -100, 16, -100, 16, 16, 16, -100, 16, -100, 16, -100, 16, 16, -100, 16, 10, 15, -100, 15, -100, 16, 16, 16, 16, 16, 16, 6, 5, 5, 16, 16, -100, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, -100, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, -100, -100, 16, 10, 15, -100, 16, 10, 15, -100, 15, -100, 15, 15, 16, 16, -100, 16, -100, 16, -100, 16, -100, 16, -100, 16, 16, 16, 16, 16, 16, 16, -100, 16, 16, -100, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, -100, 16, 16, 16, 16, 16, 16, 16, 16, -100, 16, -100, 16, -100, 16, -100, -100, 16, 16, 16, -100, 16, 16, -100, 16, 16, 16, 16, 16, 16, -100, 16, 16, 16, -100, 16, -100, 16, -100, 16, 16, 16, 16, 16, 16, 16, 16, -100, 16, 6, 16, 16, 10, 15, -100, 16, -100, 16, 16, -100, 16, 16, 16, 10, 15, -100, 15, -100, 15, 15, 16, -100, 16, 16, -100, -100, -100, 16, 16, -100, 16, 16, -100, 16, -100, -100, 16, -100, 16, 16, 16, 16, 16, 16, 16, -100, 16, -100, 16, -100, 16, 16, 16, 16, -100, 16, 16, 16, 16, 16, -100, 16, 16, 16, 16, 16, -100, 16, 16, -100, 16, 16, 16, 10, 15, -100, 16, -100, 16, 16, 16, 16, 16, 16, 16, 16, 10, 15, -100, 15, -100, 16, 16, 16, 16, -100, -100, 16, 16, 16, 16, -100, 16, -100, 16, -100, 16, -100, 16, -100, 16, 16, 16, -100, 16, 16, 16, 16, 16, 16, 16, 6, 16, 16, 16, -100, 16, 16, 16, 16, 16, 16, 16, 16, -100, 16, -100, 16, -100, 16, -100, -100, 16, 16, -100, 16, -100, 16, 16, -100, 16, 16, 16, -100, 16, 16, 16, 16, 16, 16, 6, 16, 6, 16, 16, -100, -100, 16, -100, 16, -100, 16, 16, -100, 16, -100, 16, 16, 16, 16, 16, 16, 16, 16, -100, 16, -100, 16, -100, 16, 16, 16, 16, 16, 16, 16, 16, 16, -100, 16, 10, 15, -100, 16, 14, -100]\n",
      "==========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process dataset with tokenizer and align labels\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        #padding=\"max_length\",\n",
    "        #max_length=512\n",
    "    )\n",
    "    labels = []\n",
    "    \n",
    "    for i, label in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Get mapping of subwords to words\n",
    "        previous_word_id = None\n",
    "        label_ids = []\n",
    "        \n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:  # Ignore special tokens (CLS, SEP, PAD)\n",
    "                label_ids.append(-100)\n",
    "            elif word_id != previous_word_id:  # Assign correct label only to first subword\n",
    "                label_ids.append(label[word_id])\n",
    "            else:\n",
    "                label_ids.append(-100)  # Assign -100 to subsequent subwords\n",
    "            \n",
    "            previous_word_id = word_id\n",
    "\n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    tokenized_inputs[\"decoded\"] = [[tokenizer.decode(i) for i in ids] for ids in tokenized_inputs.input_ids]\n",
    "\n",
    "    # 🔹 Debugging Step: Print a Sample to Verify Alignment\n",
    "    print(\"\\n==== Sample Debugging ====\")\n",
    "    print(\"Len examples: \", len(examples[\"tokens\"][-1]))\n",
    "    print(\"Len tokenized inputs: \", len(tokenized_inputs[\"input_ids\"][-1]))\n",
    "    print(\"Original Tokens: \", examples[\"tokens\"][-1])\n",
    "    print(\"decoded: \", tokenized_inputs[\"decoded\"][-1])\n",
    "    print(\"Tokenized Input IDs: \", tokenized_inputs[\"input_ids\"][-1])\n",
    "    print(\"Word IDs Mapping: \", word_ids)\n",
    "    print(\"Aligned Labels: \", labels[-1])\n",
    "    print(\"==========================\\n\")\n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "# Convert dataset to Hugging Face Dataset format\n",
    "dataset = Dataset.from_dict({\"tokens\": [entry[\"tokens\"] for entry in data], \"labels\": [entry[\"labels\"] for entry in data]})\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[27456], [4238, 442]], 'attention_mask': [[1], [1, 1]]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\" Nuclear\", \"waste\"], add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metric\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_labels = [[label for label in label_seq if label != -100] for label_seq in labels]\n",
    "    true_predictions = [[pred for pred, lab in zip(pred_seq, label_seq) if lab != -100] for pred_seq, label_seq in zip(predictions, labels)]\n",
    "    return seqeval.compute(predictions=true_predictions, references=true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",     # Output directory\n",
    "    eval_strategy=\"epoch\", # Evaluate after each epoch\n",
    "    save_strategy=\"epoch\",      # Save model after each epoch\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",       # Log directory\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,         # Save last 2 models only\n",
    "    fp16=True,                  # Enable mixed precision training for better performance\n",
    "    bf16=torch.cuda.is_bf16_supported(),  # Use BF16 if supported for better speed\n",
    "    optim=\"adamw_torch_fused\",  # Use fused optimizer for better CUDA performance\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magnu\\AppData\\Local\\Temp\\ipykernel_10116\\695041789.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided []",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ner_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2480\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2478\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2479\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[1;32m-> 2480\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2481\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[0;32m   2482\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:5153\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[1;34m(self, epoch_iterator, num_batches)\u001b[0m\n\u001b[0;32m   5151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[0;32m   5152\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 5153\u001b[0m         batch_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m   5154\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   5155\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\accelerate\\data_loader.py:563\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 563\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\data\\data_collator.py:45\u001b[0m, in \u001b[0;36mDataCollatorMixin.__call__\u001b[1;34m(self, features, return_tensors)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtf_call(features)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy_call(features)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\data\\data_collator.py:333\u001b[0m, in \u001b[0;36mDataCollatorForTokenClassification.torch_call\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    329\u001b[0m labels \u001b[38;5;241m=\u001b[39m [feature[label_name] \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features] \u001b[38;5;28;01mif\u001b[39;00m label_name \u001b[38;5;129;01min\u001b[39;00m features[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    331\u001b[0m no_labels_features \u001b[38;5;241m=\u001b[39m [{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m feature\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m label_name} \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[1;32m--> 333\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_labels_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\data\\data_collator.py:66\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[1;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 66\u001b[0m     padded \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m warning_state\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3305\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[1;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[0;32m   3303\u001b[0m \u001b[38;5;66;03m# The model's main input name, usually `input_ids`, has been passed for padding\u001b[39;00m\n\u001b[0;32m   3304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m encoded_inputs:\n\u001b[1;32m-> 3305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3307\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthat includes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but you provided \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(encoded_inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3308\u001b[0m     )\n\u001b[0;32m   3310\u001b[0m required_input \u001b[38;5;241m=\u001b[39m encoded_inputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m   3312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m required_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(required_input, Sized) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(required_input) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided []"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(\"./ner_model\")\n",
    "tokenizer.save_pretrained(\"./ner_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

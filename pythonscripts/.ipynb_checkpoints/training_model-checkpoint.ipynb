{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flash-attn\n",
      "  Using cached flash_attn-2.7.4.post1.tar.gz (6.0 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\magnu\\\\AppData\\\\Local\\\\Temp\\\\pip-install-_h0lj359\\\\flash-attn_a64e364bab4b4bda827435c2541f476a\\\\csrc\\\\composable_kernel\\\\library\\\\include\\\\ck\\\\library\\\\tensor_operation_instance\\\\gpu\\\\grouped_conv_bwd_weight\\\\device_grouped_conv_bwd_weight_two_stage_xdl_instance.hpp'\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#! pip install seqeval\n",
    "#! pip install evaluate\n",
    "#! pip install pandas\n",
    "#! pip install datasets\n",
    "#! pip install torch\n",
    "#! pip install transformers\n",
    "#! pip install scikit-learn\n",
    "#! pip install ninja\n",
    "#! pip install flash-attn\n",
    "#! pip install packaging\n",
    "#! pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import evaluate\n",
    "import seqeval\n",
    "import accelerate\n",
    "import transformers\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from flash_attn.flash_attention import FlashAttention\n",
    "# Flash Attention for faster training\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # Enable TF32 for better performance\n",
    "torch.backends.cudnn.benchmark = True  # Enable CuDNN auto-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "DATA_PATH = \"tokenized_ner_data_4.json\"\n",
    "with open(DATA_PATH, \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all unique label types from dataset\n",
    "unique_labels = list(set(label for entry in data for label in entry[\"labels\"]))\n",
    "\n",
    "# Create mapping from label name -> index\n",
    "label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
    "\n",
    "# Convert dataset labels from strings to integer IDs\n",
    "for entry in data:\n",
    "    entry[\"labels\"] = [label_to_id[label] for label in entry[\"labels\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    tokenized_inputs = []\n",
    "    tokenized_labels = []\n",
    "\n",
    "    for entry in data:\n",
    "        tokens = entry[\"tokens\"]\n",
    "        labels = entry[\"labels\"]\n",
    "\n",
    "        tokenized_inputs.append(tokens)\n",
    "        tokenized_labels.append(labels)\n",
    "    return {\"tokens\": tokenized_inputs, \"labels\": tokenized_labels}\n",
    "\n",
    "dataset = Dataset.from_dict(preprocess_data(data))\n",
    "train_dataset, eval_dataset = dataset.train_test_split(test_size=0.2).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForTokenClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "MODEL_NAME = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, num_labels=len(set([label for entry in data for label in entry[\"labels\"]]))).to(device)\n",
    "\n",
    "# Enable Flash Attention in the model (if applicable)\n",
    "#model.config.use_flash_attention = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2309fecef85b40378bdf36e30fbd3276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/61 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Sample Debugging ====\n",
      "Original Tokens:  ['[CLS]', 'A', 'new', 'ransom', '##ware', '-', 'as', '-', 'a', '-', 'service', '(', 'Ra', '##a', '##S', ')', 'operation', 'named', 'C', '##ica', '##da', '##33', '##01', 'has', 'already', 'listed', '19', 'victims', 'on', 'its', 'ex', '##tor', '##tion', 'portal', ',', 'as', 'it', 'quickly', 'attacked', 'companies', 'worldwide', '.', 'The', 'new', 'c', '##y', '##ber', '##c', '##rim', '##e', 'operation', 'is', 'named', 'after', 'the', 'mysterious', '2012', '-', '2014', 'online', '/', 'real', '-', 'world', 'game', 'that', 'involved', 'elaborate', 'cry', '##pt', '##ographic', 'puzzles', 'and', 'used', 'the', 'same', 'logo', 'for', 'promotion', 'on', 'c', '##y', '##ber', '##c', '##rim', '##e', 'forums', '.', 'However', ',', 'there', \"'\", 's', 'no', 'connection', 'between', 'the', 'two', ',', 'and', 'the', 'legitimate', 'project', 'has', 'issued', 'a', 'statement', 'to', 're', '##nounce', 'any', 'association', 'and', 'con', '##de', '##m', '##n', 'the', 'ransom', '##ware', 'operators', \"'\", 'actions', '.', 'The', 'C', '##ica', '##da', '##33', '##01', 'Ra', '##a', '##S', 'first', 'began', 'promoting', 'the', 'operation', 'and', 'recruiting', 'affiliates', 'on', 'June', '29', ',', '202', '##4', ',', 'in', 'a', 'forum', 'post', 'to', 'the', 'ransom', '##ware', 'and', 'c', '##y', '##ber', '##c', '##rim', '##e', 'forum', 'known', 'as', 'RAM', '##P', '.', 'However', ',', 'B', '##lee', '##ping', '##C', '##om', '##pute', '##r', 'is', 'aware', 'of', 'C', '##ica', '##da', 'attacks', 'as', 'early', 'as', 'June', '6', ',', 'indicating', 'that', 'the', 'gang', 'was', 'operating', 'independently', 'before', 'attempting', 'to', 'recruit', 'affiliates', '.', 'Like', 'other', 'ransom', '##ware', 'operations', ',', 'C', '##ica', '##da', '##33', '##01', 'conducts', 'double', '-', 'ex', '##tor', '##tion', 'tactics', 'where', 'they', 'breach', 'corporate', 'networks', ',', 'steal', 'data', ',', 'and', 'then', 'en', '##c', '##ry', '##pt', 'devices', '.', 'The', 'encryption', 'key', 'and', 'threats', 'to', 'leak', 'stolen', 'data', 'are', 'then', 'used', 'as', 'leverage', 'to', 'scare', 'victims', 'into', 'paying', 'a', 'ransom', '.', 'The', 'threat', 'actors', 'operate', 'a', 'data', 'leak', 'site', 'that', 'is', 'used', 'as', 'part', 'of', 'their', 'double', '-', 'ex', '##tor', '##tion', 'scheme', '.', 'An', 'analysis', 'of', 'the', 'new', 'ma', '##l', '##ware', 'by', 'True', '##se', '##c', 'revealed', 'significant', 'overlap', '##s', 'between', 'C', '##ica', '##da', '##33', '##01', 'and', 'AL', '##P', '##H', '##V', '/', 'Black', '##C', '##at', ',', 'indicating', 'a', 'possible', 're', '##brand', 'or', 'a', 'fork', 'created', 'by', 'former', 'AL', '##P', '##H', '##V', \"'\", 's', 'core', 'team', 'members', '.', 'This', 'is', 'based', 'on', 'the', 'fact', 'that', ':', 'For', 'context', ',', 'AL', '##P', '##H', '##V', 'performed', 'an', 'exit', 's', '##cam', 'in', 'early', 'March', '202', '##4', 'involving', 'fake', 'claims', 'about', 'an', 'FBI', 'take', '##down', 'operation', 'after', 'they', 'stole', 'a', 'massive', '$', '22', 'million', 'payment', 'from', 'Change', 'Healthcare', 'from', 'one', 'of', 'their', 'affiliates', '.', 'True', '##se', '##c', 'has', 'also', 'found', 'indication', '##s', 'that', 'the', 'C', '##ica', '##da', '##33', '##01', 'ransom', '##ware', 'operation', 'may', 'partner', 'with', 'or', 'utilize', 'the', 'B', '##ru', '##tus', 'b', '##ot', '##net', 'for', 'initial', 'access', 'to', 'corporate', 'networks', '.', 'That', 'b', '##ot', '##net', 'was', 'previously', 'associated', 'with', 'global', '-', 'scale', 'VP', '##N', 'br', '##ute', '-', 'forcing', 'activities', 'targeting', 'C', '##isco', ',', 'Fort', '##inet', ',', 'Pa', '##lo', 'Alto', ',', 'and', 'Sonic', '##W', '##all', 'appliances', '.', 'It', \"'\", 's', 'worth', 'noting', 'that', 'the', 'B', '##ru', '##tus', 'activity', 'was', 'first', 'spotted', 'two', 'weeks', 'after', 'AL', '##P', '##H', '##V', 'shut', 'down', 'operations', ',', 'so', 'the', 'link', 'between', 'the', 'two', 'groups', 'still', 'stands', 'in', 'terms', 'of', 'timeline', '##s', '.', 'C', '##ica', '##da', '##33', '##01', 'is', 'a', 'R', '##ust', '-', 'based', '[SEP]']\n",
      "Tokenized Input IDs:  [50281, 50281, 34, 1826, 16147, 297, 817, 1935, 14, 284, 14, 66, 14, 10613, 9, 21328, 817, 66, 817, 52, 10, 20936, 19389, 36, 817, 3737, 817, 1473, 817, 1610, 817, 520, 7110, 39735, 24328, 746, 40312, 14381, 251, 953, 911, 817, 13473, 817, 19606, 631, 267, 13, 284, 262, 32600, 314, 1595, 16970, 32729, 447, 10186, 4363, 15, 510, 1826, 68, 817, 90, 817, 589, 817, 68, 817, 3428, 817, 70, 20936, 261, 19389, 6438, 783, 2577, 2971, 784, 6755, 14, 6759, 27381, 16, 6549, 14, 10186, 13197, 3529, 7821, 5336, 293, 2735, 366, 43583, 817, 431, 817, 5576, 11113, 44426, 395, 3197, 783, 18941, 29402, 1542, 13382, 5011, 251, 68, 817, 90, 817, 589, 817, 68, 817, 3428, 817, 70, 1542, 7640, 15, 6436, 13, 9088, 8, 84, 2369, 14477, 17352, 783, 9389, 13, 395, 783, 1851, 42858, 10408, 7110, 47401, 66, 25322, 936, 250, 817, 79, 12943, 1279, 10769, 318, 395, 585, 817, 615, 817, 78, 817, 79, 783, 16147, 297, 817, 1935, 2211, 2392, 8, 3518, 15, 510, 36, 817, 3737, 817, 1473, 817, 1610, 817, 520, 21328, 817, 66, 817, 52, 7053, 67, 30558, 13382, 5341, 783, 20936, 395, 2845, 579, 2996, 2843, 3093, 684, 251, 18948, 1717, 13, 18161, 817, 21, 13, 249, 66, 39061, 5996, 936, 783, 16147, 297, 817, 1935, 395, 68, 817, 90, 817, 589, 817, 68, 817, 3428, 817, 70, 39061, 4304, 284, 37231, 817, 49, 15, 6436, 13, 35, 817, 14906, 817, 14650, 817, 36, 817, 297, 817, 48334, 817, 83, 261, 13823, 1171, 36, 817, 3737, 817, 1473, 1595, 7305, 284, 18579, 284, 18948, 23, 13, 527, 30782, 3529, 783, 26774, 4238, 2211, 839, 527, 2008, 1574, 9131, 38839, 272, 936, 2845, 5527, 2843, 3093, 684, 15, 9817, 977, 16147, 297, 817, 1935, 42316, 13, 36, 817, 3737, 817, 1473, 817, 1610, 817, 520, 11018, 84, 12237, 14, 911, 817, 13473, 817, 19606, 85, 514, 982, 2811, 9328, 3381, 607, 46723, 366, 3024, 4896, 13, 3241, 267, 2203, 13, 395, 7461, 257, 817, 68, 817, 610, 817, 431, 49242, 15, 510, 2083, 18008, 2364, 395, 26039, 84, 936, 282, 518, 296, 12901, 2203, 609, 7461, 3197, 284, 282, 2394, 936, 1026, 609, 40312, 14381, 14806, 12080, 272, 66, 16147, 297, 15, 510, 26039, 46435, 2211, 366, 66, 2203, 282, 518, 11264, 3529, 261, 3197, 284, 2003, 1171, 14094, 12237, 14, 911, 817, 13473, 817, 19606, 29619, 15, 1145, 12792, 1171, 783, 1826, 785, 817, 77, 817, 1935, 1615, 5088, 817, 339, 817, 68, 38198, 3256, 32258, 1189, 30520, 817, 84, 17352, 36, 817, 3737, 817, 1473, 817, 1610, 817, 520, 395, 1556, 817, 49, 817, 41, 817, 55, 16, 15383, 817, 36, 817, 255, 13, 527, 30782, 66, 24902, 250, 817, 22374, 263, 66, 43491, 22337, 1615, 19946, 1556, 817, 49, 817, 41, 817, 55, 8, 84, 6443, 22035, 25011, 15, 1552, 261, 3169, 251, 783, 12690, 3529, 27, 2214, 8882, 13, 1556, 817, 49, 817, 41, 817, 55, 468, 10574, 266, 19874, 84, 817, 12583, 249, 18579, 18489, 18161, 817, 21, 7821, 11932, 39182, 28803, 10383, 266, 39, 50282]\n",
      "Word IDs Mapping:  [None, 0, 1, 1, 2, 2, 3, 4, 4, 5, 6, 6, 7, 7, 8, 9, 10, 10, 11, 12, 12, 13, 13, 14, 15, 16, 17, 18, 19, 19, 20, 21, 22, 23, 24, 25, 25, 26, 26, 27, 28, 29, 29, 30, 31, 32, 32, 33, 34, 35, 35, 36, 37, 38, 38, 39, 39, 40, 40, 41, 41, 42, 42, 43, 44, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 58, 59, 60, 61, 62, 63, 64, 64, 65, 65, 65, 66, 67, 68, 68, 69, 69, 70, 71, 72, 73, 74, 75, 76, 77, 77, 78, 78, 79, 79, 80, 80, 81, 81, 82, 83, 83, 84, 85, 86, 86, 87, 87, 88, 88, 89, 89, 89, 90, 91, 92, 93, 94, 95, 95, 96, 96, 96, 97, 98, 98, 99, 100, 100, 101, 101, 102, 103, 103, 104, 105, 106, 107, 108, 108, 108, 109, 110, 110, 111, 112, 113, 113, 113, 114, 115, 116, 116, 117, 118, 118, 119, 120, 121, 121, 122, 122, 123, 124, 125, 125, 126, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 135, 136, 137, 138, 138, 139, 140, 141, 142, 143, 143, 144, 144, 145, 145, 146, 147, 148, 149, 149, 150, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 159, 160, 161, 162, 163, 164, 164, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 174, 175, 176, 177, 177, 178, 179, 180, 181, 182, 183, 183, 184, 185, 186, 187, 188, 189, 190, 190, 191, 191, 192, 193, 194, 195, 196, 196, 197, 198, 199, 199, 200, 200, 201, 201, 202, 203, 203, 204, 205, 206, 207, 208, 209, 209, 210, 210, 210, 211, 212, 212, 213, 213, 214, 215, 216, 216, 217, 218, 218, 219, 219, 220, 221, 222, 223, 224, 224, 225, 225, 226, 227, 227, 228, 229, 229, 230, 230, 231, 232, 233, 233, 234, 235, 236, 237, 237, 238, 238, 239, 240, 241, 241, 242, 242, 243, 243, 244, 245, 245, 246, 247, 248, 248, 249, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 282, 282, 283, 284, 285, 285, 286, 287, 288, 288, 289, 289, 290, 291, 292, 293, 293, 294, 294, 295, 295, 296, 296, 297, 297, 298, 299, 300, 301, 302, 303, 304, 304, 305, 306, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 323, 324, 325, 326, 327, 328, 329, 330, 331, 331, 332, 332, 333, 333, 334, 334, 334, 335, 336, 337, 337, 338, 339, 339, 340, 341, 342, 343, 344, 345, 345, 346, 347, 348, 348, 349, 349, 350, 350, 351, 352, 353, 354, 355, 356, 357, 358, 358, 359, 360, 361, 362, 363, 364, 364, 365, 365, 366, 367, 367, 368, 369, 370, 371, 372, 372, 373, 373, 374, 375, 376, 376, 377, 378, 378, None]\n",
      "Aligned Labels:  [-100, 7, 7, 7, 7, -100, 7, -100, 7, 7, 7, 7, 7, 7, 7, 7, 7, -100, 7, -100, 7, 7, 7, 0, 18, -100, 18, -100, 18, -100, 18, -100, 7, 7, 7, 7, 7, -100, 7, 7, 7, 7, -100, 7, -100, 7, -100, 7, 7, 7, 7, -100, 7, -100, 7, -100, 7, -100, 7, 7, 7, 7, 7, -100, 7, -100, 7, -100, 7, -100, 7, -100, 7, 7, 7, 7, 7, 7, -100, -100, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, -100, 7, -100, -100, 7, 7, -100, 7, -100, 7, -100, 7, 7, 7, 7, 7, 7, 7, -100, 7, 7, 7, -100, 7, -100, 7, -100, 7, -100, 7, -100, 7, -100, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, -100, 7, 7, 7, 7, 7, 7, 7, 7, -100, -100, 7, 7, -100, 7, 7, 7, -100, 7, -100, 7, -100, 7, 7, -100, 7, -100, 7, -100, 7, 7, 7, 7, 0, 18, -100, 18, -100, 18, -100, 18, -100, 7, 7, -100, 7, -100, 7, 7, -100, 7, -100, 7, 7, 7, 7, -100, -100, 7, -100, -100, 7, 7, 7, 7, 7, 7, -100, 7, 7, 7, 7, 7, 7, 7, 7, -100, 7, -100, 7, 7, 7, -100, 7, -100, 7, -100, 7, -100, 7, -100, 7, 7, 7, 7, 7, -100, 7, 7, 7, 16, 2, -100, 2, -100, 2, -100, 2, -100, 2, -100, 2, -100, 7, 7, 7, 0, 18, -100, 18, -100, 7, -100, 7, 7, 7, 7, 7, 7, 7, -100, 7, 7, 7, 7, 7, -100, 7, -100, -100, 7, 7, -100, 7, 7, -100, 7, -100, -100, 7, 7, 7, 7, -100, 7, -100, 7, 7, 0, 18, -100, 18, -100, 18, -100, 18, -100, 4, -100, 1, 1, 1, 1, -100, 1, -100, 1, -100, -100, 1, 1, 1, -100, 1, -100, 1, -100, 1, 1, -100, 1, 1, 1, 1, 1, 1, -100, 1, -100, 1, -100, 1, 7, 7, 7, -100, 7, 7, 7, -100, 7, 7, -100, 7, -100, 7, 7, 7, 7, 7, 7, -100, 7, 7, -100, 7, -100, 7, 7, -100, 7, 7, -100, 7, 7, 7, 7, 7, -100, 7, 7, 7, -100, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, -100, 7, -100, 7, 7, 7, 7, 7, 7, 7, 7, 7, -100, 7, -100, 7, 16, 2, -100, 2, -100, 7, -100, 7, 7, -100, 7, -100, 7, 0, 18, -100, 18, -100, 18, -100, 18, -100, 7, 0, 18, -100, 18, -100, 18, -100, 7, 0, 18, -100, 18, -100, 7, 7, -100, 7, 7, 7, 7, -100, 7, 7, 7, 7, 7, 7, 0, 18, -100, 18, -100, 18, -100, 18, 18, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 0, 18, -100, 18, -100, 18, -100, 7, -100, 7, 7, 7, 7, -100, 7, 7, 7, 7, 7, -100, 7, -100, 7, 7, 7, 7, 16, -100]\n",
      "==========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process dataset with tokenizer and align labels\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "    labels = []\n",
    "    \n",
    "    for i, label in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Get mapping of subwords to words\n",
    "        previous_word_id = None\n",
    "        label_ids = []\n",
    "        \n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:  # Ignore special tokens (CLS, SEP, PAD)\n",
    "                label_ids.append(-100)\n",
    "            elif word_id != previous_word_id:  # Assign correct label only to first subword\n",
    "                label_ids.append(label[word_id])\n",
    "            else:\n",
    "                label_ids.append(-100)  # Assign -100 to subsequent subwords\n",
    "            \n",
    "            previous_word_id = word_id\n",
    "\n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "\n",
    "    # 🔹 Debugging Step: Print a Sample to Verify Alignment\n",
    "    print(\"\\n==== Sample Debugging ====\")\n",
    "    print(\"Original Tokens: \", examples[\"tokens\"][0])\n",
    "    print(\"Tokenized Input IDs: \", tokenized_inputs[\"input_ids\"][0])\n",
    "    print(\"Word IDs Mapping: \", word_ids)\n",
    "    print(\"Aligned Labels: \", labels[0])\n",
    "    print(\"==========================\\n\")\n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "# Convert dataset to Hugging Face Dataset format\n",
    "dataset = Dataset.from_dict({\"tokens\": [entry[\"tokens\"] for entry in data], \"labels\": [entry[\"labels\"] for entry in data]})\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metric\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_labels = [[label for label in label_seq if label != -100] for label_seq in labels]\n",
    "    true_predictions = [[pred for pred, lab in zip(pred_seq, label_seq) if lab != -100] for pred_seq, label_seq in zip(predictions, labels)]\n",
    "    return seqeval.compute(predictions=true_predictions, references=true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",     # Output directory\n",
    "    eval_strategy=\"epoch\", # Evaluate after each epoch\n",
    "    save_strategy=\"epoch\",      # Save model after each epoch\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",       # Log directory\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,         # Save last 2 models only\n",
    "    fp16=True,                  # Enable mixed precision training for better performance\n",
    "    bf16=torch.cuda.is_bf16_supported(),  # Use BF16 if supported for better speed\n",
    "    optim=\"adamw_torch_fused\",  # Use fused optimizer for better CUDA performance\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magnu\\AppData\\Local\\Temp\\ipykernel_10116\\695041789.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided []",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ner_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2480\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2478\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2479\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[1;32m-> 2480\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2481\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[0;32m   2482\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:5153\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[1;34m(self, epoch_iterator, num_batches)\u001b[0m\n\u001b[0;32m   5151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[0;32m   5152\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 5153\u001b[0m         batch_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m   5154\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   5155\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\accelerate\\data_loader.py:563\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 563\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\data\\data_collator.py:45\u001b[0m, in \u001b[0;36mDataCollatorMixin.__call__\u001b[1;34m(self, features, return_tensors)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtf_call(features)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy_call(features)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\data\\data_collator.py:333\u001b[0m, in \u001b[0;36mDataCollatorForTokenClassification.torch_call\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    329\u001b[0m labels \u001b[38;5;241m=\u001b[39m [feature[label_name] \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features] \u001b[38;5;28;01mif\u001b[39;00m label_name \u001b[38;5;129;01min\u001b[39;00m features[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    331\u001b[0m no_labels_features \u001b[38;5;241m=\u001b[39m [{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m feature\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m label_name} \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[1;32m--> 333\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_labels_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\data\\data_collator.py:66\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[1;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 66\u001b[0m     padded \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m warning_state\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3305\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[1;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[0;32m   3303\u001b[0m \u001b[38;5;66;03m# The model's main input name, usually `input_ids`, has been passed for padding\u001b[39;00m\n\u001b[0;32m   3304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m encoded_inputs:\n\u001b[1;32m-> 3305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3307\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthat includes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but you provided \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(encoded_inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3308\u001b[0m     )\n\u001b[0;32m   3310\u001b[0m required_input \u001b[38;5;241m=\u001b[39m encoded_inputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m   3312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m required_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(required_input, Sized) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(required_input) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided []"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(\"./ner_model\")\n",
    "tokenizer.save_pretrained(\"./ner_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
